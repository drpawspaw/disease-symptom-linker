{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBlPtNmsDGgc"
      },
      "source": [
        "## Install package and download required data files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOmdtzKADFVP",
        "outputId": "5cdda274-8e02-4170-8016-3eda83020e0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-01-08 11:37:07--  https://raw.githubusercontent.com/viraj-lakshitha/animal-disease-symptom-ontology/develop/ADSOv1.0.3.owl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68571 (67K) [text/plain]\n",
            "Saving to: ‘ADSOv1.0.3.owl’\n",
            "\n",
            "ADSOv1.0.3.owl      100%[===================>]  66.96K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-01-08 11:37:07 (4.57 MB/s) - ‘ADSOv1.0.3.owl’ saved [68571/68571]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download ADSO ontology file\n",
        "!wget \"https://raw.githubusercontent.com/viraj-lakshitha/animal-disease-symptom-ontology/develop/ADSOv1.0.3.owl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9s81WWgDe0A",
        "outputId": "ef614ace-ccc0-4322-9cc6-24fe4b8fa6f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rdflib\n",
            "  Downloading rdflib-6.2.0-py3-none-any.whl (500 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.3/500.3 KB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting isodate\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from rdflib) (3.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from rdflib) (57.4.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.13.0+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.14.0+cu116)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub) (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub) (3.8.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from isodate->rdflib) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=6804da4a1da5f16bd17e51ff2aad0693e243e1e6cc56750bab114adec8fe7e41\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, isodate, rdflib, huggingface-hub, transformers, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.11.1 isodate-0.6.1 rdflib-6.2.0 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install rdflib spacy sentence-transformers huggingface-hub transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4v4SEn8D0RA"
      },
      "source": [
        "## Load required pre-trained models\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SI807hKlEbDy",
        "outputId": "ea5b0e39-d486-4c85-d7a7-53c7b4bb8827"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download Wordnet dependencies\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        },
        "id": "InmZXdRODnc5",
        "outputId": "66f283c7-906e-4cc2-81f4-66e5349baa4d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ddec763ea5d04735a7ea8f74dc907d34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d1a39056dbc4daab4f57d4e62c2edaf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e9122cd4d564580a3944481a3c1d746",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7b070e71e20465aaf164d2d9a1c19b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6067ebd4eb14984816ed428b74c6221",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "383ed0c6e6ed4870aab84af8d866de69",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "773f77660db84f739efa23ed29fb4cb7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5897f254ba94855804eaa89e9d4fb77",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d37e43b35c94b35b9d7bb5150b0cd80",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "407f88011e2c4d49bb4bea7a3845d62e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0dcce1a0eedb4e0aa5fe3ff892e10a16",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7edb83c0e63c43ed91b932b643042571",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7333af6f6b4f43299ce9bab73dec28c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d80c4b0ef5849cc84ebf26bfd19fb70",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "config.json not found in HuggingFace Hub\n",
            "WARNING:huggingface_hub.hub_mixin:config.json not found in HuggingFace Hub\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04f5f2d954fc44618d43bd67572f6496",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a407d7c1f5ac49df905c23f10d0d7627",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.36k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9d0c44fdaf84e46b1120fe40adcb96e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.59k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d774149b2d2942d4a41829cca0f15151",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/169k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b50f273f55b74d0fbee46adccc6b1676",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.97M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bfa06ec5bc094802ae030b982217167e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/6.47k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e068ac682d7f4d2b95287787e53dabc3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/30.0k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c2674d8e3b941dab247e3aa8042814e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/7.88M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e34284aeafa04e19975d7d8f756108e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06a1025844df4e81bda6b005b79b45ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/12.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "from huggingface_hub import from_pretrained_keras\n",
        "import numpy as np\n",
        "import transformers\n",
        "import tensorflow as tf\n",
        "\n",
        "# Text similarity model\n",
        "tsm = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2');\n",
        "\n",
        "# Semantic similarity model\n",
        "ssm = from_pretrained_keras(\"keras-io/bert-semantic-similarity\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWRSUfHdDYGl"
      },
      "source": [
        "## Helper Utils\n",
        "\n",
        "This block contains functions/classes/enums that use throught out the entity disambiguation component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjA8BQv4Dda7"
      },
      "outputs": [],
      "source": [
        "# Help utils for semantic similarity model\n",
        "class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n",
        "    \"\"\"Generates batches of data.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        sentence_pairs,\n",
        "        labels,\n",
        "        batch_size=32,\n",
        "        shuffle=True,\n",
        "        include_targets=True,\n",
        "    ):\n",
        "        self.sentence_pairs = sentence_pairs\n",
        "        self.labels = labels\n",
        "        self.shuffle = shuffle\n",
        "        self.batch_size = batch_size\n",
        "        self.include_targets = include_targets\n",
        "        # Load our BERT Tokenizer to encode the text.\n",
        "        # We will use base-base-uncased pretrained model.\n",
        "        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n",
        "            \"bert-base-uncased\", do_lower_case=True\n",
        "        )\n",
        "        self.indexes = np.arange(len(self.sentence_pairs))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Denotes the number of batches per epoch.\n",
        "        return len(self.sentence_pairs) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Retrieves the batch of index.\n",
        "        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        sentence_pairs = self.sentence_pairs[indexes]\n",
        "\n",
        "        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n",
        "        # encoded together and separated by [SEP] token.\n",
        "        encoded = self.tokenizer.batch_encode_plus(\n",
        "            sentence_pairs.tolist(),\n",
        "            add_special_tokens=True,\n",
        "            max_length=128,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=True,\n",
        "            pad_to_max_length=True,\n",
        "            return_tensors=\"tf\",\n",
        "        )\n",
        "\n",
        "        # Convert batch of encoded features to numpy array.\n",
        "        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
        "\n",
        "        # Set to true if data generator is used for training/validation.\n",
        "        if self.include_targets:\n",
        "            labels = np.array(self.labels[indexes], dtype=\"int32\")\n",
        "            return [input_ids, attention_masks, token_type_ids], labels\n",
        "        else:\n",
        "            return [input_ids, attention_masks, token_type_ids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGU_PZl9YP4C"
      },
      "outputs": [],
      "source": [
        "labels = [\"negative_similarity\", \"positive_similarity\", \"neutral\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhUv5WvtFmRG"
      },
      "source": [
        "## Data loader component\n",
        "\n",
        "In here we are going to retreive the all type of entities/nodes from the ontology knowledgebase and store in seperate arrays.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAVqHRX9Fl3G",
        "outputId": "e7443a6f-3715-4020-fdcf-b43dd66fd655"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Graph identifier=Na135e7e596ab417389ad2fbe9ff75495 (<class 'rdflib.graph.Graph'>)>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load downloaded ontology graph\n",
        "from rdflib import Graph, Namespace, Literal, RDF, URIRef\n",
        "\n",
        "g = Graph()\n",
        "g.parse(\"/content/ADSOv1.0.3.owl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dk_hfKFRF24-"
      },
      "outputs": [],
      "source": [
        "# Get the URIRef for given keyword\n",
        "def get_ref(keyword):\n",
        "    return URIRef(\"https://ontology.drpawspaw.com/\"+keyword)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKdr7P0VF222"
      },
      "outputs": [],
      "source": [
        "# Get the entity 'text' from the URIRef\n",
        "def get_text_from_uri(uri):\n",
        "    for s,p,o in g:\n",
        "        if s == uri and p == URIRef(\"https://ontology.drpawspaw.com/text\"):\n",
        "            return o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47Zcfs0zF20V",
        "outputId": "6a940b5f-d43c-429f-84d4-a7afa39ae8c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'rdflib.term.URIRef'> https://ontology.drpawspaw.com/ADSO0000000110\n"
          ]
        }
      ],
      "source": [
        "# Collect named entities from ontology\n",
        "diseases = []\n",
        "symptoms = []\n",
        "synonyms = []\n",
        "\n",
        "for s,p,o in g:\n",
        "    if p == get_ref(\"hasDisease\"):\n",
        "        try:\n",
        "            dis = get_text_from_uri(o).toPython()\n",
        "            diseases.append(dis)\n",
        "        except:\n",
        "            print(type(o), o)\n",
        "    if p == get_ref(\"hasSymptom\"):\n",
        "        try:\n",
        "            sym = get_text_from_uri(o).toPython()\n",
        "            symptoms.append(sym)\n",
        "        except:\n",
        "            print(type(o), o)\n",
        "    if p == get_ref(\"hasSynonym\"):\n",
        "        try:\n",
        "            syn = get_text_from_uri(o).toPython()\n",
        "            synonyms.append(syn)\n",
        "        except:\n",
        "            print(type(o), o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbvkQWIVF2O2"
      },
      "outputs": [],
      "source": [
        "# List of symptoms and their synonyms\n",
        "symp_syn = []\n",
        "\n",
        "# Check the keyword already exist or not in collection\n",
        "def is_exist_ss(keyword):\n",
        "    for _,k,_ in symp_syn:\n",
        "        if k == keyword:\n",
        "            return True\n",
        "            \n",
        "# Apped all the symptoms and their synonyms\n",
        "for s,p,o in g: # subject, predicate, object\n",
        "    # filter synonyms\n",
        "    if p == get_ref(\"hasSymptom\"):\n",
        "        try:\n",
        "            x = get_text_from_uri(o).toPython()\n",
        "        except:\n",
        "            continue\n",
        "        curr_symp_syn = []\n",
        "        for s1, p1, o1 in g:\n",
        "            # filter synonym for above \"o\" entity\n",
        "            if s1 == o and p1 == get_ref(\"hasSynonym\"):\n",
        "                try:\n",
        "                    y = get_text_from_uri(o1).toPython()\n",
        "                    curr_symp_syn.append(y)                    \n",
        "                except:\n",
        "                    continue\n",
        "        # validate to add only one entry\n",
        "        if not is_exist_ss(x):\n",
        "            try:\n",
        "                idx = o.toPython()\n",
        "                symp_syn.append((idx, x, curr_symp_syn))\n",
        "            except:\n",
        "                continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxh0vvqJFlvA",
        "outputId": "a6cb2d0d-8f70-40f3-e51d-a8446d8f2d55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('https://ontology.drpawspaw.com/ADSO0000000047', 'Holding the Leg up', []),\n",
              " ('https://ontology.drpawspaw.com/ADSO0000000091', 'Weakness', []),\n",
              " ('https://ontology.drpawspaw.com/ADSO0000000076', 'Pigmentation of Skin', []),\n",
              " ('https://ontology.drpawspaw.com/ADSO0000000038',\n",
              "  'Involuntary Eye Movements',\n",
              "  []),\n",
              " ('https://ontology.drpawspaw.com/ADSO0000000017', 'Lethargy', ['Low Energy'])]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "symp_syn[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmZzuzcRHDSV",
        "outputId": "5d4df84b-1e7b-4f38-de5f-6de6b33833e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "79"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(symp_syn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfGSSJUBGvyo"
      },
      "source": [
        "## Text similarity\n",
        "\n",
        "In here we are going to calculate the similarity of the words using pretrained model from Hugging Face => Text Similarity - To get the similar words for identified named entities (https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2, https://huggingface.co/tasks/sentence-similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQ42zIs8O7B_"
      },
      "outputs": [],
      "source": [
        "# Generate synonyms for the existing symptoms and update the list of synonyms\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def generate_synonyms(uri, keyword, syns):\n",
        "    for synonym in wordnet.synsets(keyword):\n",
        "        for item in synonym.lemmas():\n",
        "            if keyword != synonym.name() and len(synonym.lemma_names()) > 1:\n",
        "                syns.append(item.name())\n",
        "    return (uri, keyword, syns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSjE0NnrRzPj"
      },
      "outputs": [],
      "source": [
        "# for idx,(uri,keyword,syns) in enumerate(symp_syn):\n",
        "#   symp_syn[idx] = generate_synonyms(uri,keyword,syns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQU9S-PdSL3K",
        "outputId": "6378a584-b6c2-416f-d2aa-50dc4f789398"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "79\n",
            "[('https://ontology.drpawspaw.com/ADSO0000000047', 'Holding the Leg up', []), ('https://ontology.drpawspaw.com/ADSO0000000091', 'Weakness', []), ('https://ontology.drpawspaw.com/ADSO0000000076', 'Pigmentation of Skin', []), ('https://ontology.drpawspaw.com/ADSO0000000038', 'Involuntary Eye Movements', []), ('https://ontology.drpawspaw.com/ADSO0000000017', 'Lethargy', ['Low Energy'])]\n"
          ]
        }
      ],
      "source": [
        "print(len(symp_syn))\n",
        "print(symp_syn[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpgG4VkkOjRE"
      },
      "outputs": [],
      "source": [
        "# Exapand the all entities\n",
        "expanded_symp = []\n",
        "\n",
        "def is_exist_es(keyword):\n",
        "    for idx,word in expanded_symp:\n",
        "        if keyword == word:\n",
        "            return True\n",
        "\n",
        "for idx,word,syns in symp_syn:\n",
        "    for s in syns:\n",
        "        if not is_exist_es(s):\n",
        "            expanded_symp.append((idx, s))\n",
        "    if not is_exist_es(word):\n",
        "        expanded_symp.append((idx, word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G5MdZKBPLuS",
        "outputId": "dd914832-85d5-468e-e987-9a52478bb7ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "101\n",
            "[('https://ontology.drpawspaw.com/ADSO0000000047', 'Holding the Leg up'), ('https://ontology.drpawspaw.com/ADSO0000000091', 'Weakness'), ('https://ontology.drpawspaw.com/ADSO0000000076', 'Pigmentation of Skin'), ('https://ontology.drpawspaw.com/ADSO0000000038', 'Involuntary Eye Movements'), ('https://ontology.drpawspaw.com/ADSO0000000017', 'Low Energy')]\n"
          ]
        }
      ],
      "source": [
        "print(len(expanded_symp))\n",
        "print(expanded_symp[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_xdYqyvG_hI"
      },
      "outputs": [],
      "source": [
        "# Calculate the similariity\n",
        "# Here \"tsm\" means the text-similarity-model, that we loaded in \"Load required pre-trained models\"\n",
        "# \"get_similarity\" accepts the URI, identified named entity, entity from ontology\n",
        "def get_similarity(idx, word, ne): # Return a tuple, contain URI, node, similarity_score\n",
        "    embedding_1 = tsm.encode(ne, convert_to_tensor=True)\n",
        "    embedding_2 = tsm.encode(word, convert_to_tensor=True)\n",
        "    return (idx, word, util.pytorch_cos_sim(embedding_1, embedding_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRNcOcRpS4Bx"
      },
      "outputs": [],
      "source": [
        "# At the current implementation, we only get first five entities\n",
        "# \"entity\" - identified named entity, \"nodes\" - named entities in the onotology (in format of [(idx, text)])\n",
        "def get_most_similarity_entities(entity, nodes):\n",
        "  scores = map(lambda e: get_similarity(e[0], e[1], entity), nodes)\n",
        "  return sorted(list(scores), key=lambda x:x[2], reverse=True)[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2_OOmWaRJS7",
        "outputId": "2cca2049-06bc-4b9c-a3d0-b1ede7833095"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('https://ontology.drpawspaw.com/ADSO0000000005', 'Fever', tensor([[1.]])),\n",
              " ('https://ontology.drpawspaw.com/ADSO0000000041',\n",
              "  'High Fever',\n",
              "  tensor([[0.8284]])),\n",
              " ('https://ontology.drpawspaw.com/ADSO0000000032',\n",
              "  'Pneumonia',\n",
              "  tensor([[0.4948]])),\n",
              " ('https://ontology.drpawspaw.com/ADSO0000000041',\n",
              "  'High Temperature',\n",
              "  tensor([[0.4413]])),\n",
              " ('https://ontology.drpawspaw.com/ADSO0000000008',\n",
              "  'Vomiting',\n",
              "  tensor([[0.4372]]))]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Tryout the implementation\n",
        "identified_ne = [\"fever\", \"lethargy\"]\n",
        "\n",
        "# Get the top five entities from the text similarity, accroding to the cosine similarity score\n",
        "get_most_similarity_entities(identified_ne[0], expanded_symp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6ilvSv6XtzP"
      },
      "source": [
        "## Semantic Similarity\n",
        "\n",
        "In here we are going to calculate the semantic similarity of the words using pretrained model from HuggingFace => Semantic Similarity - To get the similarity in context for the ranked candidates (https://huggingface.co/keras-io/bert-semantic-similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WPFjjDKX-Z-"
      },
      "outputs": [],
      "source": [
        "# Calculate the semantic similarity of the identified entity and nodes in ontology\n",
        "# \"calculate_semantic_similarity\" functions accpets, the URI, text and identified entity\n",
        "def calculate_semantic_similarity(uri, ctx, entity):\n",
        "    sentence_pairs = np.array([[str(entity), str(ctx)]])\n",
        "    test_data = BertSemanticDataGenerator(\n",
        "        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n",
        "    )\n",
        "    probs = ssm.predict(test_data[0])[0]\n",
        "    labels_probs = {labels[i]: float(probs[i]) for i, _ in enumerate(labels)}\n",
        "    return (uri, ctx, labels_probs[\"positive_similarity\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHozNg4DYkaq"
      },
      "outputs": [],
      "source": [
        "# \"get_highest_ctx_similarity\" function return the highest context similarity as tuple (URI, Word, ctx_similarity)\n",
        "def get_highest_ctx_similarity(entity, nodes):\n",
        "  scores = map(lambda e: calculate_semantic_similarity(e[0], e[1], entity), nodes)\n",
        "  return sorted(list(scores), key=lambda x:x[2], reverse=True)[:1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c2hx2W25piI"
      },
      "source": [
        "## Tryout implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ndR0sB6YoDd",
        "outputId": "365e368a-b279-4ecd-feaa-dec9b927d71e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 966ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 953ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 565ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 564ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 571ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('https://ontology.drpawspaw.com/ADSO0000000017',\n",
              "  'Lethargy',\n",
              "  0.9999686479568481)]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ine = \"fever\"\n",
        "candidates = get_most_similarity_entities(ine, expanded_symp) # Candidates from text-similarity\n",
        "get_highest_ctx_similarity(ine, candidates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci4b6Q6-IyJX"
      },
      "source": [
        "## Query data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "rYLK3UFeI3Bd"
      },
      "outputs": [],
      "source": [
        "# This function return SPARQL query, that able to get the disease from symptoms\n",
        "def build_query(*symptoms):\n",
        "  return \"\"\"\n",
        "    PREFIX adso: <https://ontology.drpawspaw.com/>\n",
        "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
        "    SELECT ?diseaseName\n",
        "    WHERE {{\n",
        "        {sym_query}\n",
        "        ?diseaseUri adso:text ?diseaseName .\n",
        "    }}\n",
        "    \"\"\".format(sym_query=\"\\n\".join(\n",
        "        list(\n",
        "            map(lambda e: \"?diseaseUri adso:hasSymptom adso:{uri} .\".format(uri=e),\n",
        "                list(map(lambda e: e.split(\"/\")[3], symptoms))))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgQxHsybJ2s9",
        "outputId": "7b30ef6d-0447-449a-9f80-6de6b3496bac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(rdflib.term.Literal('Babesiosis', lang='en'),)\n"
          ]
        }
      ],
      "source": [
        "q = build_query(\"https://ontology.drpawspaw.com/ADSO0000000005\",\n",
        "    \"https://ontology.drpawspaw.com/ADSO0000000006\",\n",
        "    \"https://ontology.drpawspaw.com/ADSO0000000007\")\n",
        "\n",
        "for output in g.query(q):\n",
        "  print(output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "sBlPtNmsDGgc",
        "V4v4SEn8D0RA",
        "yWRSUfHdDYGl",
        "RhUv5WvtFmRG",
        "9c2hx2W25piI"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}