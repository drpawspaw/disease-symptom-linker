{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBlPtNmsDGgc"
      },
      "source": [
        "## Install package and download required data files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOmdtzKADFVP",
        "outputId": "ae596f5c-b9a6-49a8-f740-bc80ef81fbef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-08 10:14:38--  https://raw.githubusercontent.com/viraj-lakshitha/animal-disease-symptom-ontology/develop/ADSOv1.0.3.owl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68571 (67K) [text/plain]\n",
            "Saving to: ‘ADSOv1.0.3.owl.1’\n",
            "\n",
            "ADSOv1.0.3.owl.1    100%[===================>]  66.96K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-03-08 10:14:38 (4.50 MB/s) - ‘ADSOv1.0.3.owl.1’ saved [68571/68571]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download ADSO ontology file\n",
        "!wget \"https://raw.githubusercontent.com/viraj-lakshitha/animal-disease-symptom-ontology/develop/ADSOv1.0.3.owl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9s81WWgDe0A",
        "outputId": "8663bac0-02d6-4621-d2fb-a6c0815d0433"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.8/dist-packages (6.2.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.8/dist-packages (2.2.2)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.8/dist-packages (0.12.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.8/dist-packages (3.20.1)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.8/dist-packages (from rdflib) (0.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from rdflib) (57.4.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from rdflib) (3.0.9)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.1.97)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.14.1+cu116)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.2.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.8/dist-packages (from gradio) (0.3.0)\n",
            "Requirement already satisfied: websockets>=10.0 in /usr/local/lib/python3.8/dist-packages (from gradio) (10.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from gradio) (2023.1.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.8/dist-packages (from gradio) (0.23.3)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.8/dist-packages (from gradio) (0.0.6)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.8/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from gradio) (2.2.0)\n",
            "Requirement already satisfied: mdit-py-plugins<=0.3.3 in /usr/local/lib/python3.8/dist-packages (from gradio) (0.3.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.8/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.8/dist-packages (from gradio) (0.93.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from gradio) (3.8.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from gradio) (8.4.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.8/dist-packages (from gradio) (2.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from gradio) (3.5.3)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.8/dist-packages (from gradio) (3.17)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.8/dist-packages (from gradio) (3.8.7)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.8/dist-packages (from gradio) (23.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from gradio) (1.3.5)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->gradio) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.8/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.8/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->gradio) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (3.0.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (6.0.4)\n",
            "Requirement already satisfied: starlette<0.26.0,>=0.25.0 in /usr/local/lib/python3.8/dist-packages (from fastapi->gradio) (0.25.0)\n",
            "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /usr/local/lib/python3.8/dist-packages (from httpx->gradio) (0.16.3)\n",
            "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /usr/local/lib/python3.8/dist-packages (from httpx->gradio) (1.5.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.8/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from isodate->rdflib) (1.15.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (4.38.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.8/dist-packages (from uvicorn->gradio) (0.14.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.8/dist-packages (from httpcore<0.17.0,>=0.15.0->httpx->gradio) (3.6.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (5.12.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.8/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio) (1.0.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema>=3.0->altair>=4.2.0->gradio) (3.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install rdflib spacy sentence-transformers huggingface-hub transformers gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4v4SEn8D0RA"
      },
      "source": [
        "## Load required pre-trained models\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SI807hKlEbDy",
        "outputId": "e22d1549-cc90-4468-dec9-2327cb8a5602"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "# Download Wordnet dependencies\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Get a list of English stop words\n",
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "id": "vEOIIWsDS-w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InmZXdRODnc5",
        "outputId": "7da5373f-fe40-44ba-994e-e6a4c7b4ea1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:299: UserWarning: Trying to unpickle estimator DictVectorizer from version 1.0.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:299: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.0.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:299: UserWarning: Trying to unpickle estimator Pipeline from version 1.0.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "from huggingface_hub import from_pretrained_keras\n",
        "import numpy as np\n",
        "import transformers\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "\n",
        "# POS tagger\n",
        "pos_tagger = joblib.load(\"/content/pos-tagger.joblib\")\n",
        "\n",
        "# Text similarity model\n",
        "tsm = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2');\n",
        "\n",
        "# Semantic similarity model\n",
        "ssm = SentenceTransformer('Maite89/Roberta_finetuning_semantic_similarity_stsb_multi_mt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions to define the properties of the pos-tagger model\n",
        "def features(sentence, index):\n",
        "    return {\n",
        "        'word': sentence[index],\n",
        "        'is_first': index == 0,\n",
        "        'is_last': index == len(sentence) - 1,\n",
        "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
        "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
        "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
        "        'prefix-1': sentence[index][0],\n",
        "        'prefix-2': sentence[index][:2],\n",
        "        'prefix-3': sentence[index][:3],\n",
        "        'suffix-1': sentence[index][-1],\n",
        "        'suffix-2': sentence[index][-2:],\n",
        "        'suffix-3': sentence[index][-3:],\n",
        "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
        "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
        "        'has_hyphen': '-' in sentence[index],\n",
        "        'is_numeric': sentence[index].isdigit(),\n",
        "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
        "    }"
      ],
      "metadata": {
        "id": "AXsZfqdwWJsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhUv5WvtFmRG"
      },
      "source": [
        "## Data loader component\n",
        "\n",
        "In here we are going to retreive the all type of entities/nodes from the ontology knowledgebase and store in seperate arrays.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAVqHRX9Fl3G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a54e417-8ada-4bf5-8a16-17edfffda318"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Graph identifier=N3db6d694e9d7452c83e51e517255b14f (<class 'rdflib.graph.Graph'>)>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "# Load downloaded ontology graph\n",
        "from rdflib import Graph, Namespace, Literal, RDF, URIRef\n",
        "\n",
        "g = Graph()\n",
        "g.parse(\"/content/ADSOv1.0.3.owl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dk_hfKFRF24-"
      },
      "outputs": [],
      "source": [
        "# Get the URIRef for given keyword\n",
        "def get_ref(keyword):\n",
        "    return URIRef(\"https://ontology.drpawspaw.com/\"+keyword)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKdr7P0VF222"
      },
      "outputs": [],
      "source": [
        "# Get the entity 'text' from the URIRef\n",
        "def get_text_from_uri(uri):\n",
        "    for s,p,o in g:\n",
        "        if s == uri and p == URIRef(\"https://ontology.drpawspaw.com/text\"):\n",
        "            return o\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_text_from_uri(URIRef('https://ontology.drpawspaw.com/ADSO0000000083')))"
      ],
      "metadata": {
        "id": "qKBVN22L472b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "092b78db-027b-43a7-8d17-641a4c8c0666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distempter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list of symptoms from disease URI\n",
        "def get_symptoms_from_disease_uri(uri):\n",
        "  symptoms = []\n",
        "  for s,p,o in g:\n",
        "    if s == uri and p == URIRef(\"https://ontology.drpawspaw.com/hasSymptom\"):\n",
        "      symptoms.append(o.toPython())\n",
        "  return symptoms"
      ],
      "metadata": {
        "id": "5QAAdeY47KNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_symptoms_from_disease_uri(URIRef('https://ontology.drpawspaw.com/ADSO0000000083'))"
      ],
      "metadata": {
        "id": "ok6CoQJ27hEX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "869ec2d7-3753-4ea5-f3c0-4dfcc07c10a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://ontology.drpawspaw.com/ADSO0000000039',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000036',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000031',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000093',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000037',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000007',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000026',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000032',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000033',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000030',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000008',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000040',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000023',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000038',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000027',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000017',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000034',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000028',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000009',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000005']"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get URI from text\n",
        "def get_uri_from_text(text):\n",
        "  for s, p, o in g:\n",
        "    if p == URIRef(\"https://ontology.drpawspaw.com/text\"):\n",
        "      if o.toPython().lower() == text.lower():\n",
        "        return s\n",
        "  return None"
      ],
      "metadata": {
        "id": "bEy_tsWC34mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_uri_from_text(\"fever\")"
      ],
      "metadata": {
        "id": "0cD6ogjF4LPV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bd699d6-e4f4-4cf2-b242-ba4d168a7abe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "rdflib.term.URIRef('https://ontology.drpawspaw.com/ADSO0000000005')"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47Zcfs0zF20V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd4ced9e-9d8a-4933-9314-b8b2cbb04d6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'rdflib.term.URIRef'> https://ontology.drpawspaw.com/ADSO0000000110\n"
          ]
        }
      ],
      "source": [
        "# Collect named entities from ontology\n",
        "diseases = []\n",
        "symptoms = []\n",
        "synonyms = []\n",
        "\n",
        "for s,p,o in g:\n",
        "    if p == get_ref(\"hasDisease\"):\n",
        "        try:\n",
        "            dis = get_text_from_uri(o).toPython()\n",
        "            diseases.append(dis)\n",
        "        except:\n",
        "            print(type(o), o)\n",
        "    if p == get_ref(\"hasSymptom\"):\n",
        "        try:\n",
        "            sym = get_text_from_uri(o).toPython()\n",
        "            symptoms.append(sym)\n",
        "        except:\n",
        "            print(type(o), o)\n",
        "    if p == get_ref(\"hasSynonym\"):\n",
        "        try:\n",
        "            syn = get_text_from_uri(o).toPython()\n",
        "            synonyms.append(syn)\n",
        "        except:\n",
        "            print(type(o), o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbvkQWIVF2O2"
      },
      "outputs": [],
      "source": [
        "# List of symptoms and their synonyms\n",
        "symp_syn = []\n",
        "\n",
        "# Check the keyword already exist or not in collection\n",
        "def is_exist_ss(keyword):\n",
        "    for _,k,_ in symp_syn:\n",
        "        if k == keyword:\n",
        "            return True\n",
        "            \n",
        "# Apped all the symptoms and their synonyms\n",
        "for s,p,o in g: # subject, predicate, object\n",
        "    # filter synonyms\n",
        "    if p == get_ref(\"hasSymptom\"):\n",
        "        try:\n",
        "            x = get_text_from_uri(o).toPython()\n",
        "        except:\n",
        "            continue\n",
        "        curr_symp_syn = []\n",
        "        for s1, p1, o1 in g:\n",
        "            # filter synonym for above \"o\" entity\n",
        "            if s1 == o and p1 == get_ref(\"hasSynonym\"):\n",
        "                try:\n",
        "                    y = get_text_from_uri(o1).toPython()\n",
        "                    curr_symp_syn.append(y)                    \n",
        "                except:\n",
        "                    continue\n",
        "        # validate to add only one entry\n",
        "        if not is_exist_ss(x):\n",
        "            try:\n",
        "                idx = o.toPython()\n",
        "                symp_syn.append((idx, x, curr_symp_syn))\n",
        "            except:\n",
        "                continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxh0vvqJFlvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d57baf6d-5907-4a15-ec76-8c8d7fe20421"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('https://ontology.drpawspaw.com/ADSO0000000012', 'Stroke', []),\n",
              " ('https://ontology.drpawspaw.com/ADSO0000000039', 'Paralysis', []),\n",
              " ('https://ontology.drpawspaw.com/ADSO0000000042', 'Running Nose', []),\n",
              " ('https://ontology.drpawspaw.com/ADSO0000000026',\n",
              "  'Nasal Discharge',\n",
              "  ['Nasal Whistling', 'Nasal Wheezing']),\n",
              " ('https://ontology.drpawspaw.com/ADSO0000000074', 'Skin Scaling', [])]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "symp_syn[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmZzuzcRHDSV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1deaa7c7-5370-4ef7-892e-971d2ebffd7a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "len(symp_syn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfGSSJUBGvyo"
      },
      "source": [
        "## Text similarity\n",
        "\n",
        "In here we are going to calculate the similarity of the words using pretrained model from Hugging Face => Text Similarity - To get the similar words for identified named entities (https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2, https://huggingface.co/tasks/sentence-similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQ42zIs8O7B_"
      },
      "outputs": [],
      "source": [
        "# Generate synonyms for the existing symptoms and update the list of synonyms\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def generate_synonyms(uri, keyword, syns):\n",
        "    for synonym in wordnet.synsets(keyword):\n",
        "        for item in synonym.lemmas():\n",
        "            if keyword != synonym.name() and len(synonym.lemma_names()) > 1:\n",
        "                syns.append(item.name())\n",
        "    return (uri, keyword, syns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSjE0NnrRzPj"
      },
      "outputs": [],
      "source": [
        "# for idx,(uri,keyword,syns) in enumerate(symp_syn):\n",
        "#   symp_syn[idx] = generate_synonyms(uri,keyword,syns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQU9S-PdSL3K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed4e81de-4536-4ae6-c819-9420719ae0fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79\n",
            "[('https://ontology.drpawspaw.com/ADSO0000000012', 'Stroke', []), ('https://ontology.drpawspaw.com/ADSO0000000039', 'Paralysis', []), ('https://ontology.drpawspaw.com/ADSO0000000042', 'Running Nose', []), ('https://ontology.drpawspaw.com/ADSO0000000026', 'Nasal Discharge', ['Nasal Whistling', 'Nasal Wheezing']), ('https://ontology.drpawspaw.com/ADSO0000000074', 'Skin Scaling', [])]\n"
          ]
        }
      ],
      "source": [
        "print(len(symp_syn))\n",
        "print(symp_syn[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpgG4VkkOjRE"
      },
      "outputs": [],
      "source": [
        "# Exapand the all entities\n",
        "expanded_symp = []\n",
        "\n",
        "def is_exist_es(keyword):\n",
        "    for idx,word in expanded_symp:\n",
        "        if keyword == word:\n",
        "            return True\n",
        "\n",
        "for idx,word,syns in symp_syn:\n",
        "    for s in syns:\n",
        "        if not is_exist_es(s):\n",
        "            expanded_symp.append((idx, s))\n",
        "    if not is_exist_es(word):\n",
        "        expanded_symp.append((idx, word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-G5MdZKBPLuS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8f856ab-ef73-46ea-f381-32d60f9bceea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101\n",
            "[('https://ontology.drpawspaw.com/ADSO0000000012', 'Stroke'), ('https://ontology.drpawspaw.com/ADSO0000000039', 'Paralysis'), ('https://ontology.drpawspaw.com/ADSO0000000042', 'Running Nose'), ('https://ontology.drpawspaw.com/ADSO0000000026', 'Nasal Whistling'), ('https://ontology.drpawspaw.com/ADSO0000000026', 'Nasal Wheezing')]\n"
          ]
        }
      ],
      "source": [
        "print(len(expanded_symp))\n",
        "print(expanded_symp[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_xdYqyvG_hI"
      },
      "outputs": [],
      "source": [
        "# Calculate the similariity\n",
        "# Here \"tsm\" means the text-similarity-model, that we loaded in \"Load required pre-trained models\"\n",
        "# \"get_similarity\" accepts the URI, identified named entity, entity from ontology\n",
        "def get_similarity(idx, word, ne): # Return a tuple, contain URI, node, similarity_score\n",
        "    embedding_1 = tsm.encode(ne, convert_to_tensor=True)\n",
        "    embedding_2 = tsm.encode(word, convert_to_tensor=True)\n",
        "    return (idx, word, util.pytorch_cos_sim(embedding_1, embedding_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRNcOcRpS4Bx"
      },
      "outputs": [],
      "source": [
        "# At the current implementation, we only get first five entities\n",
        "# \"entity\" - identified named entity, \"nodes\" - named entities in the onotology (in format of [(idx, text)])\n",
        "def get_most_similarity_entities(entity, nodes):\n",
        "  scores = map(lambda e: get_similarity(e[0], e[1], entity), nodes)\n",
        "  return sorted(list(scores), key=lambda x:x[2], reverse=True)[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2_OOmWaRJS7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e58f4f4-9e63-4b45-d7e3-3dd87bafea8d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('https://ontology.drpawspaw.com/ADSO0000000005', 'Fever', tensor([[1.]])),\n",
              " ('https://ontology.drpawspaw.com/ADSO0000000041',\n",
              "  'High Fever',\n",
              "  tensor([[0.8284]])),\n",
              " ('https://ontology.drpawspaw.com/ADSO0000000032',\n",
              "  'Pneumonia',\n",
              "  tensor([[0.4948]])),\n",
              " ('https://ontology.drpawspaw.com/ADSO0000000041',\n",
              "  'High Temperature',\n",
              "  tensor([[0.4413]])),\n",
              " ('https://ontology.drpawspaw.com/ADSO0000000008',\n",
              "  'Vomiting',\n",
              "  tensor([[0.4372]]))]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "# Tryout the implementation\n",
        "identified_ne = [\"fever\", \"lethargy\"]\n",
        "\n",
        "# Get the top five entities from the text similarity, accroding to the cosine similarity score\n",
        "get_most_similarity_entities(identified_ne[0], expanded_symp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6ilvSv6XtzP"
      },
      "source": [
        "## Semantic Similarity\n",
        "\n",
        "In here we are going to calculate the semantic similarity of the words using pretrained model from HuggingFace => Text/Paragraph and Sentence similarity - This model especially designed to check the semantic similarity of biological related things (https://huggingface.co/pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb)\n",
        "\n",
        "```\n",
        "Example:\n",
        "- Percentage similarity between 'fever' and 'cough' is 72.12%\n",
        "- Percentage similarity between 'fever' and 'high body temperature' is 76.15%\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WPFjjDKX-Z-"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Calculate the semantic similarity of the identified entity and nodes in ontology\n",
        "# \"calculate_semantic_similarity\" functions accpets, the URI, text and identified entity\n",
        "def calculate_semantic_similarity(uri, ctx, entity):\n",
        "    sent_embed = ssm.encode([entity, ctx])\n",
        "    # Compute cosine similarity between the two sentence embeddings\n",
        "    similarity = cosine_similarity([sent_embed[0]], [sent_embed[1]])[0][0]\n",
        "    # Convert cosine similarity to percentage similarity\n",
        "    percentage_similarity = round((similarity + 1) / 2 * 100, 2)\n",
        "    return (uri, ctx, percentage_similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHozNg4DYkaq"
      },
      "outputs": [],
      "source": [
        "# \"get_highest_ctx_similarity\" function return the highest context similarity as tuple (URI, Word, ctx_similarity)\n",
        "def get_highest_ctx_similarity(entity, nodes):\n",
        "  scores = map(lambda e: calculate_semantic_similarity(e[0], e[1], entity), nodes)\n",
        "  return sorted(list(scores), key=lambda x:x[2], reverse=True)[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ndR0sB6YoDd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0816f818-8cab-4416-8368-c1d158798c1f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('https://ontology.drpawspaw.com/ADSO0000000005', 'Fever', 88.27)]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "ine = \"fever\"\n",
        "candidates = get_most_similarity_entities(ine, expanded_symp) # Candidates from text-similarity\n",
        "get_highest_ctx_similarity(ine, candidates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci4b6Q6-IyJX"
      },
      "source": [
        "## Query data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYLK3UFeI3Bd"
      },
      "outputs": [],
      "source": [
        "# This function return SPARQL query, that able to get the disease from symptoms\n",
        "def build_query(symptoms):\n",
        "  return \"\"\"\n",
        "    PREFIX adso: <https://ontology.drpawspaw.com/>\n",
        "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
        "    SELECT ?diseaseName\n",
        "    WHERE {{\n",
        "        {sym_query}\n",
        "        ?diseaseUri adso:text ?diseaseName .\n",
        "    }}\n",
        "    \"\"\".format(sym_query=\"\\n\".join(\n",
        "        list(\n",
        "            map(lambda e: \"?diseaseUri adso:hasSymptom adso:{uri} .\".format(uri=e),\n",
        "                list(map(lambda e: e.split(\"/\")[3], symptoms))))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgQxHsybJ2s9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2aad8d0-4d68-461c-f616-470b63e84e34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(rdflib.term.Literal('Babesiosis', lang='en'),)\n"
          ]
        }
      ],
      "source": [
        "q = build_query([\"https://ontology.drpawspaw.com/ADSO0000000005\",\n",
        "    \"https://ontology.drpawspaw.com/ADSO0000000006\",\n",
        "    \"https://ontology.drpawspaw.com/ADSO0000000007\"])\n",
        "\n",
        "for output in g.query(q):\n",
        "  print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation tryout"
      ],
      "metadata": {
        "id": "xFVfyOFvSwR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing inputs"
      ],
      "metadata": {
        "id": "hp4vB2bgjsqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing (remove stop words)\n",
        "# user_input = \"My dog has been vomiting has Limb Swelling and has diarrhea\"\n",
        "user_input = \"My dog has been vomiting and has diarrhea \"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = nltk.word_tokenize(user_input)\n",
        "\n",
        "# Remove stop words, (stopwords are getting fromt the NLTK library)\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "# Filtered tokens as sentence\n",
        "filtered_user_input = \" \".join(filtered_tokens)"
      ],
      "metadata": {
        "id": "fpPPSzXlSvse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_user_input"
      ],
      "metadata": {
        "id": "vCtQ5yiJV9-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44df8c78-bc87-48d5-b2b7-4fde2284bad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dog vomiting diarrhea'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "def pos_tag(sentence):\n",
        "    tags = pos_tagger.predict([features(sentence, index) for index in range(len(sentence))])\n",
        "    return zip(sentence, tags)\n",
        "\n",
        "annotated_entites = [en for en in list(\n",
        "      pos_tag(word_tokenize(filtered_user_input)) # Filter entities annotate as \"SYMPTOM\"\n",
        "    ) if en[1] == \"SYMPTOM\"]"
      ],
      "metadata": {
        "id": "KCBSszIgV_0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotated_entites"
      ],
      "metadata": {
        "id": "6K3wqM0cXyPz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9528ba5-bccf-4614-f2e7-cf0086ad16f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('vomiting', 'SYMPTOM'), ('diarrhea', 'SYMPTOM')]"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing entity linker component"
      ],
      "metadata": {
        "id": "Ob8ZkNGbj10B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "link_entity = []\n",
        "\n",
        "for entity in annotated_entites:\n",
        "  current_entity_candidates = get_most_similarity_entities(entity[0], expanded_symp)\n",
        "  link_entity.append(get_highest_ctx_similarity(entity[0], current_entity_candidates))"
      ],
      "metadata": {
        "id": "LrOtqWTLYjKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "link_entity"
      ],
      "metadata": {
        "id": "FIUkY60Xb2xd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7be55df-d3fb-42e1-91c9-04efbb44f314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('https://ontology.drpawspaw.com/ADSO0000000008', 'Vomiting', 98.06)],\n",
              " [('https://ontology.drpawspaw.com/ADSO0000000009', 'Diarrhea', 97.38)]]"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query symptoms to get the disease"
      ],
      "metadata": {
        "id": "3LZ1xnuVj-JH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entities = list(map(lambda x: x[0][0], link_entity))"
      ],
      "metadata": {
        "id": "oyZZnlXhZqLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entities"
      ],
      "metadata": {
        "id": "j_u6x4P5h2_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac216cfe-3103-4733-d989-b248e37f64c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://ontology.drpawspaw.com/ADSO0000000008',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000009']"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "curr_query = build_query(entities)"
      ],
      "metadata": {
        "id": "3h-2S0Y4btmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "curr_query"
      ],
      "metadata": {
        "id": "JaH0yGv_b7up",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "862ac71a-c7ee-4ad1-ae5c-2278f4d6eeab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    PREFIX adso: <https://ontology.drpawspaw.com/>\\n    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\\n    SELECT ?diseaseName\\n    WHERE {\\n        ?diseaseUri adso:hasSymptom adso:ADSO0000000008 .\\n?diseaseUri adso:hasSymptom adso:ADSO0000000009 .\\n        ?diseaseUri adso:text ?diseaseName .\\n    }\\n    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for rq in g.query(curr_query):\n",
        "  print(rq)"
      ],
      "metadata": {
        "id": "5nLu2YRqZ2F6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19045355-e0b9-4dd2-e6f1-589e4399ba17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(rdflib.term.Literal('Babesiosis', lang='en'),)\n",
            "(rdflib.term.Literal('Distempter', lang='en'),)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WORD_JOIN_CHAR = \" \"; # Use to join the words in an array\n",
        "\n",
        "from nltk import word_tokenize\n",
        "\n",
        "def pos_tag(sentence):\n",
        "    tags = pos_tagger.predict([features(sentence, index) for index in range(len(sentence))])\n",
        "    return zip(sentence, tags)\n",
        "\n",
        "def entity_linker(sentence):\n",
        "  etq = [] # Entities to query\n",
        "  annotated_entites = [en for en in list(\n",
        "      pos_tag(word_tokenize(sentence)) # Filter entities annotate as \"SYMPTOM\"\n",
        "    ) if en[1] == \"SYMPTOM\"]\n",
        "  print(\"Identified named entities => \", annotated_entites, '\\n')\n",
        "  if len(annotated_entites) == 0:\n",
        "    return []\n",
        "  for e in annotated_entites:\n",
        "    current_entity_candidates = get_most_similarity_entities(e[0], expanded_symp)\n",
        "    print(\"Candidates => \",e, current_entity_candidates)\n",
        "    etq.append(get_highest_ctx_similarity(e, current_entity_candidates))\n",
        "  print(\"\\nNamed Entity => \", etq)\n",
        "  current_query = build_query(list(map(lambda x: x[0][0], etq)))\n",
        "  print(\"\\nQuery => \", current_query)\n",
        "  return [rq for rq in g.query(current_query)]"
      ],
      "metadata": {
        "id": "Di8KXw6Tw1Cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entity_linker(\"My dog has been vomiting has Limb Swelling and has diarrhea\")"
      ],
      "metadata": {
        "id": "TLn2fQdgxzco",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73a4aa0b-85dd-4628-c334-41979cfb069a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified named entities =>  [('vomiting', 'SYMPTOM'), ('Swelling', 'SYMPTOM'), ('diarrhea', 'SYMPTOM')] \n",
            "\n",
            "Candidates =>  ('vomiting', 'SYMPTOM') [('https://ontology.drpawspaw.com/ADSO0000000008', 'Vomiting', tensor([[1.0000]])), ('https://ontology.drpawspaw.com/ADSO0000000057', 'Nausea', tensor([[0.7699]])), ('https://ontology.drpawspaw.com/ADSO0000000009', 'Diarrhea', tensor([[0.6183]])), ('https://ontology.drpawspaw.com/ADSO0000000066', 'Eating Poison', tensor([[0.5464]])), ('https://ontology.drpawspaw.com/ADSO0000000007', 'coughing', tensor([[0.5324]]))]\n",
            "Candidates =>  ('Swelling', 'SYMPTOM') [('https://ontology.drpawspaw.com/ADSO0000000048', 'Swelling Limbs', tensor([[0.7699]])), ('https://ontology.drpawspaw.com/ADSO0000000014', 'Limb Swelling', tensor([[0.7656]])), ('https://ontology.drpawspaw.com/ADSO0000000100', 'Swelling of Ear Canals', tensor([[0.5785]])), ('https://ontology.drpawspaw.com/ADSO0000000015', 'Heavy Pain', tensor([[0.5241]])), ('https://ontology.drpawspaw.com/ADSO0000000055', 'Bruising', tensor([[0.5092]]))]\n",
            "Candidates =>  ('diarrhea', 'SYMPTOM') [('https://ontology.drpawspaw.com/ADSO0000000009', 'Diarrhea', tensor([[1.]])), ('https://ontology.drpawspaw.com/ADSO0000000016', 'Blood Diarrhea', tensor([[0.7598]])), ('https://ontology.drpawspaw.com/ADSO0000000008', 'Vomiting', tensor([[0.6183]])), ('https://ontology.drpawspaw.com/ADSO0000000057', 'Nausea', tensor([[0.6028]])), ('https://ontology.drpawspaw.com/ADSO0000000059', 'Bloated Tummy', tensor([[0.4899]]))]\n",
            "\n",
            "Named Entity =>  [[('https://ontology.drpawspaw.com/ADSO0000000009', 'Diarrhea', 64.32)], [('https://ontology.drpawspaw.com/ADSO0000000100', 'Swelling of Ear Canals', 75.47)], [('https://ontology.drpawspaw.com/ADSO0000000009', 'Diarrhea', 66.22)]]\n",
            "\n",
            "Query =>  \n",
            "    PREFIX adso: <https://ontology.drpawspaw.com/>\n",
            "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
            "    SELECT ?diseaseName\n",
            "    WHERE {\n",
            "        ?diseaseUri adso:hasSymptom adso:ADSO0000000009 .\n",
            "?diseaseUri adso:hasSymptom adso:ADSO0000000100 .\n",
            "?diseaseUri adso:hasSymptom adso:ADSO0000000009 .\n",
            "        ?diseaseUri adso:text ?diseaseName .\n",
            "    }\n",
            "    \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain\n",
        "\n",
        "def make_prediction(sentence):\n",
        "  prediction = entity_linker(sentence)\n",
        "  # prediction = [(rdflib.term.Literal('Babesiosis', lang='en'),), (rdflib.term.Literal('Distempter', lang='en'),)]\n",
        "  # If there are multiple predictions from the given symptoms\n",
        "  if len(prediction) > 1:\n",
        "    # Retrieve the URI for above prediction\n",
        "    print(\"prediction\", prediction)\n",
        "    duris = list(map(lambda x: get_uri_from_text(x[0].toPython()).toPython(), prediction))\n",
        "    # Get the all unique symptoms for predicted diseases\n",
        "    symptoms_uris = list(map(lambda x: get_symptoms_from_disease_uri(URIRef(x)), list(duris)))\n",
        "    print(\"symptoms_uris\", symptoms_uris)\n",
        "    # Convert back to symptoms into readable format\n",
        "    symptom_suggestions = list(map(lambda x: get_text_from_uri(URIRef(x)).toPython(), list(chain.from_iterable(symptoms_uris))))\n",
        "    print(\"symptom_suggestions\", symptom_suggestions)\n",
        "    # Return symptoms suggestions for the better prediction\n",
        "    return {\n",
        "      \"request\": sentence,\n",
        "      \"result_type\": \"SUGGESTION\",\n",
        "      \"symptom_suggestions\": list(set(symptom_suggestions)),\n",
        "      \"predicted_disease\": None\n",
        "    }\n",
        "  # Unable to predict\n",
        "  if len(prediction) < 1:\n",
        "    return {\n",
        "      \"request\": sentence,\n",
        "      \"result_type\": \"LIMITATION\",\n",
        "      \"symptom_suggestions\": None,\n",
        "      \"predicted_disease\": None\n",
        "    }\n",
        "  # Return the prediction\n",
        "  result = {\n",
        "      \"request\": sentence,\n",
        "      \"result_type\": \"PREDICTION\",\n",
        "      \"symptom_suggestions\": None,\n",
        "      \"predicted_disease\": prediction[0]\n",
        "  }\n",
        "  return result"
      ],
      "metadata": {
        "id": "U9_eodgk2GLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "print(make_prediction('My dog has been vomiting has Limb Swelling and has diarrhea'))\n",
        "print(\"--- Time elaped for single execution: \", time.time()-start_time, \" ---\")"
      ],
      "metadata": {
        "id": "K2bXQUnpGW7x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31a71c1f-1aa8-48e7-ec43-cc300d6b8ae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified named entities =>  [('vomiting', 'SYMPTOM'), ('Swelling', 'SYMPTOM'), ('diarrhea', 'SYMPTOM')] \n",
            "\n",
            "Candidates =>  ('vomiting', 'SYMPTOM') [('https://ontology.drpawspaw.com/ADSO0000000008', 'Vomiting', tensor([[1.0000]])), ('https://ontology.drpawspaw.com/ADSO0000000057', 'Nausea', tensor([[0.7699]])), ('https://ontology.drpawspaw.com/ADSO0000000009', 'Diarrhea', tensor([[0.6183]])), ('https://ontology.drpawspaw.com/ADSO0000000066', 'Eating Poison', tensor([[0.5464]])), ('https://ontology.drpawspaw.com/ADSO0000000007', 'coughing', tensor([[0.5324]]))]\n",
            "Candidates =>  ('Swelling', 'SYMPTOM') [('https://ontology.drpawspaw.com/ADSO0000000048', 'Swelling Limbs', tensor([[0.7699]])), ('https://ontology.drpawspaw.com/ADSO0000000014', 'Limb Swelling', tensor([[0.7656]])), ('https://ontology.drpawspaw.com/ADSO0000000100', 'Swelling of Ear Canals', tensor([[0.5785]])), ('https://ontology.drpawspaw.com/ADSO0000000015', 'Heavy Pain', tensor([[0.5241]])), ('https://ontology.drpawspaw.com/ADSO0000000055', 'Bruising', tensor([[0.5092]]))]\n",
            "Candidates =>  ('diarrhea', 'SYMPTOM') [('https://ontology.drpawspaw.com/ADSO0000000009', 'Diarrhea', tensor([[1.]])), ('https://ontology.drpawspaw.com/ADSO0000000016', 'Blood Diarrhea', tensor([[0.7598]])), ('https://ontology.drpawspaw.com/ADSO0000000008', 'Vomiting', tensor([[0.6183]])), ('https://ontology.drpawspaw.com/ADSO0000000057', 'Nausea', tensor([[0.6028]])), ('https://ontology.drpawspaw.com/ADSO0000000059', 'Bloated Tummy', tensor([[0.4899]]))]\n",
            "\n",
            "Named Entity =>  [[('https://ontology.drpawspaw.com/ADSO0000000009', 'Diarrhea', 64.32)], [('https://ontology.drpawspaw.com/ADSO0000000100', 'Swelling of Ear Canals', 75.47)], [('https://ontology.drpawspaw.com/ADSO0000000009', 'Diarrhea', 66.22)]]\n",
            "\n",
            "Query =>  \n",
            "    PREFIX adso: <https://ontology.drpawspaw.com/>\n",
            "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
            "    SELECT ?diseaseName\n",
            "    WHERE {\n",
            "        ?diseaseUri adso:hasSymptom adso:ADSO0000000009 .\n",
            "?diseaseUri adso:hasSymptom adso:ADSO0000000100 .\n",
            "?diseaseUri adso:hasSymptom adso:ADSO0000000009 .\n",
            "        ?diseaseUri adso:text ?diseaseName .\n",
            "    }\n",
            "    \n",
            "{'request': 'My dog has been vomiting has Limb Swelling and has diarrhea', 'result_type': 'LIMITATION', 'symptom_suggestions': None, 'predicted_disease': None}\n",
            "--- Time elaped for single execution:  19.967615604400635  ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "r-k9ADkGrv7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create test data from the ontology and SPARQL"
      ],
      "metadata": {
        "id": "m4j9vqgM5Tua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test data set using the ontology knowledgebase\n",
        "'''\n",
        "[((symptom1, symptom2, ...), disease)]\n",
        "'''\n",
        "import random\n",
        "\n",
        "test_data = [] # 1\n",
        "while len(test_data) < 100:\n",
        "  get_random_num = random.randint(1, 3)\n",
        "  stup = []\n",
        "  for sid in range(get_random_num):\n",
        "    sidx = random.randint(0, len(symptoms)-1)\n",
        "    if get_uri_from_text(symptoms[sidx]) == None:\n",
        "      continue\n",
        "    try:\n",
        "      curr_symp = get_uri_from_text(symptoms[sidx]).toPython()\n",
        "      stup.append(curr_symp)\n",
        "    except:\n",
        "      continue\n",
        "  q = build_query(stup)\n",
        "  pred = [output for output in g.query(q)]\n",
        "  if len(pred) == 1:\n",
        "    test_data.append((stup, [output for output in g.query(q)]))"
      ],
      "metadata": {
        "id": "AIcrtJycrsll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[:5]"
      ],
      "metadata": {
        "id": "-VkTCB0HXPXz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da130354-8462-4c6c-972b-99d334a0a830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['https://ontology.drpawspaw.com/ADSO0000000027'],\n",
              "  [(rdflib.term.Literal('Distempter', lang='en'))]),\n",
              " (['https://ontology.drpawspaw.com/ADSO0000000040'],\n",
              "  [(rdflib.term.Literal('Distempter', lang='en'))]),\n",
              " (['https://ontology.drpawspaw.com/ADSO0000000033'],\n",
              "  [(rdflib.term.Literal('Distempter', lang='en'))]),\n",
              " (['https://ontology.drpawspaw.com/ADSO0000000098'],\n",
              "  [(rdflib.term.Literal('Ear Infections', lang='en'))]),\n",
              " (['https://ontology.drpawspaw.com/ADSO0000000074'],\n",
              "  [(rdflib.term.Literal('Demodicosis', lang='en'))])]"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform testing"
      ],
      "metadata": {
        "id": "RGR_wAua5_u8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the entity linker component and remove the pos_tag, since it already evaluated individually\n",
        "WORD_JOIN_CHAR = \" \"; # Use to join the words in an array\n",
        "\n",
        "from nltk import word_tokenize\n",
        "\n",
        "def pos_tag(sentence):\n",
        "    tags = pos_tagger.predict([features(sentence, index) for index in range(len(sentence))])\n",
        "    return zip(sentence, tags)\n",
        "\n",
        "def entity_linker_for_eval(identified):\n",
        "  etq = [] # Entities to query\n",
        "  for e in identified:\n",
        "    opt_en = [e, \"SYMPTOM\"]\n",
        "    current_entity_candidates = get_most_similarity_entities(opt_en[0], expanded_symp)\n",
        "    etq.append(get_highest_ctx_similarity(opt_en, current_entity_candidates))\n",
        "  current_query = build_query(list(map(lambda x: x[0][0], etq)))\n",
        "  return [rq for rq in g.query(current_query)]"
      ],
      "metadata": {
        "id": "572eR0LCSdbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[2]"
      ],
      "metadata": {
        "id": "Wi9EEfxGLkjk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68ac9155-23c8-440b-c534-62b800ae46ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['https://ontology.drpawspaw.com/ADSO0000000033'],\n",
              " [(rdflib.term.Literal('Distempter', lang='en'))])"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the module, predict the disease from the sentence\n",
        "# Skip the first element of the array, since it's the headers\n",
        "\"\"\"\n",
        " (\n",
        "   [\n",
        "     'https://ontology.drpawspaw.com/ADSO0000000020'\n",
        "   ],\n",
        "   [\n",
        "     (rdflib.term.Literal('Parvo', lang='en')),\n",
        "     (rdflib.term.Literal('Pneumonia', lang='en'))\n",
        "   ]\n",
        "  ),\n",
        "\"\"\"\n",
        "\n",
        "test_result = [] # (idx, symptoms ,linker_pred, actual_pred)\n",
        "for idx, sentence in enumerate(test_data[1:]):\n",
        "  print(\"\\n---------- Running test case #{test_id} ----------\".format(test_id=idx+1))\n",
        "  curr_symps = list(map(lambda x: get_text_from_uri(URIRef(x)).toPython(), sentence[0]))\n",
        "  curr_pred = [p[0].toPython() for p in entity_linker_for_eval(curr_symps)]\n",
        "  test_result.append((idx, \" \".join(curr_symps), curr_pred, [p[0].toPython() for p in sentence[1]]))"
      ],
      "metadata": {
        "id": "9nRNkyXlJCE0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429d2d3c-e316-4edd-ab53-965285eca948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---------- Running test case #1 ----------\n",
            "\n",
            "---------- Running test case #2 ----------\n",
            "\n",
            "---------- Running test case #3 ----------\n",
            "\n",
            "---------- Running test case #4 ----------\n",
            "\n",
            "---------- Running test case #5 ----------\n",
            "\n",
            "---------- Running test case #6 ----------\n",
            "\n",
            "---------- Running test case #7 ----------\n",
            "\n",
            "---------- Running test case #8 ----------\n",
            "\n",
            "---------- Running test case #9 ----------\n",
            "\n",
            "---------- Running test case #10 ----------\n",
            "\n",
            "---------- Running test case #11 ----------\n",
            "\n",
            "---------- Running test case #12 ----------\n",
            "\n",
            "---------- Running test case #13 ----------\n",
            "\n",
            "---------- Running test case #14 ----------\n",
            "\n",
            "---------- Running test case #15 ----------\n",
            "\n",
            "---------- Running test case #16 ----------\n",
            "\n",
            "---------- Running test case #17 ----------\n",
            "\n",
            "---------- Running test case #18 ----------\n",
            "\n",
            "---------- Running test case #19 ----------\n",
            "\n",
            "---------- Running test case #20 ----------\n",
            "\n",
            "---------- Running test case #21 ----------\n",
            "\n",
            "---------- Running test case #22 ----------\n",
            "\n",
            "---------- Running test case #23 ----------\n",
            "\n",
            "---------- Running test case #24 ----------\n",
            "\n",
            "---------- Running test case #25 ----------\n",
            "\n",
            "---------- Running test case #26 ----------\n",
            "\n",
            "---------- Running test case #27 ----------\n",
            "\n",
            "---------- Running test case #28 ----------\n",
            "\n",
            "---------- Running test case #29 ----------\n",
            "\n",
            "---------- Running test case #30 ----------\n",
            "\n",
            "---------- Running test case #31 ----------\n",
            "\n",
            "---------- Running test case #32 ----------\n",
            "\n",
            "---------- Running test case #33 ----------\n",
            "\n",
            "---------- Running test case #34 ----------\n",
            "\n",
            "---------- Running test case #35 ----------\n",
            "\n",
            "---------- Running test case #36 ----------\n",
            "\n",
            "---------- Running test case #37 ----------\n",
            "\n",
            "---------- Running test case #38 ----------\n",
            "\n",
            "---------- Running test case #39 ----------\n",
            "\n",
            "---------- Running test case #40 ----------\n",
            "\n",
            "---------- Running test case #41 ----------\n",
            "\n",
            "---------- Running test case #42 ----------\n",
            "\n",
            "---------- Running test case #43 ----------\n",
            "\n",
            "---------- Running test case #44 ----------\n",
            "\n",
            "---------- Running test case #45 ----------\n",
            "\n",
            "---------- Running test case #46 ----------\n",
            "\n",
            "---------- Running test case #47 ----------\n",
            "\n",
            "---------- Running test case #48 ----------\n",
            "\n",
            "---------- Running test case #49 ----------\n",
            "\n",
            "---------- Running test case #50 ----------\n",
            "\n",
            "---------- Running test case #51 ----------\n",
            "\n",
            "---------- Running test case #52 ----------\n",
            "\n",
            "---------- Running test case #53 ----------\n",
            "\n",
            "---------- Running test case #54 ----------\n",
            "\n",
            "---------- Running test case #55 ----------\n",
            "\n",
            "---------- Running test case #56 ----------\n",
            "\n",
            "---------- Running test case #57 ----------\n",
            "\n",
            "---------- Running test case #58 ----------\n",
            "\n",
            "---------- Running test case #59 ----------\n",
            "\n",
            "---------- Running test case #60 ----------\n",
            "\n",
            "---------- Running test case #61 ----------\n",
            "\n",
            "---------- Running test case #62 ----------\n",
            "\n",
            "---------- Running test case #63 ----------\n",
            "\n",
            "---------- Running test case #64 ----------\n",
            "\n",
            "---------- Running test case #65 ----------\n",
            "\n",
            "---------- Running test case #66 ----------\n",
            "\n",
            "---------- Running test case #67 ----------\n",
            "\n",
            "---------- Running test case #68 ----------\n",
            "\n",
            "---------- Running test case #69 ----------\n",
            "\n",
            "---------- Running test case #70 ----------\n",
            "\n",
            "---------- Running test case #71 ----------\n",
            "\n",
            "---------- Running test case #72 ----------\n",
            "\n",
            "---------- Running test case #73 ----------\n",
            "\n",
            "---------- Running test case #74 ----------\n",
            "\n",
            "---------- Running test case #75 ----------\n",
            "\n",
            "---------- Running test case #76 ----------\n",
            "\n",
            "---------- Running test case #77 ----------\n",
            "\n",
            "---------- Running test case #78 ----------\n",
            "\n",
            "---------- Running test case #79 ----------\n",
            "\n",
            "---------- Running test case #80 ----------\n",
            "\n",
            "---------- Running test case #81 ----------\n",
            "\n",
            "---------- Running test case #82 ----------\n",
            "\n",
            "---------- Running test case #83 ----------\n",
            "\n",
            "---------- Running test case #84 ----------\n",
            "\n",
            "---------- Running test case #85 ----------\n",
            "\n",
            "---------- Running test case #86 ----------\n",
            "\n",
            "---------- Running test case #87 ----------\n",
            "\n",
            "---------- Running test case #88 ----------\n",
            "\n",
            "---------- Running test case #89 ----------\n",
            "\n",
            "---------- Running test case #90 ----------\n",
            "\n",
            "---------- Running test case #91 ----------\n",
            "\n",
            "---------- Running test case #92 ----------\n",
            "\n",
            "---------- Running test case #93 ----------\n",
            "\n",
            "---------- Running test case #94 ----------\n",
            "\n",
            "---------- Running test case #95 ----------\n",
            "\n",
            "---------- Running test case #96 ----------\n",
            "\n",
            "---------- Running test case #97 ----------\n",
            "\n",
            "---------- Running test case #98 ----------\n",
            "\n",
            "---------- Running test case #99 ----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the accuracy/classification report\n",
        "def contain_common_elements(arr1, arr2):\n",
        "  arr1.sort()\n",
        "  arr2.sort()\n",
        "  return arr1 == arr2\n",
        "\n",
        "incorrect_count = 0\n",
        "for ts in test_result:\n",
        "  if not contain_common_elements(ts[2], ts[3]):\n",
        "    incorrect_count += 1\n",
        "\n",
        "# Accuracy of model = 1 - (number_incorrect_sample / number_of_test_samples)\n",
        "accuracy = 1 - (incorrect_count / len(test_result))\n",
        "print('Accuracy of the module is ', accuracy)"
      ],
      "metadata": {
        "id": "PUjPjW_ZJH-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7dba207-0ee2-4a82-a4e0-1c3a953f5bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the module is  0.76767676767676768\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "sBlPtNmsDGgc",
        "V4v4SEn8D0RA",
        "RhUv5WvtFmRG",
        "cfGSSJUBGvyo",
        "x6ilvSv6XtzP",
        "Ci4b6Q6-IyJX",
        "hp4vB2bgjsqr",
        "Ob8ZkNGbj10B",
        "3LZ1xnuVj-JH",
        "J9KWzYSxeFNJ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
