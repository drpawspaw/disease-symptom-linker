{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data Pre-processing"
      ],
      "metadata": {
        "id": "Yzkd7CjIJPCv"
      },
      "id": "Yzkd7CjIJPCv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download NCBI dataset from Huggingface and Re-create according to input method"
      ],
      "metadata": {
        "id": "4ezOBj7Ky_Yw"
      },
      "id": "4ezOBj7Ky_Yw"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/isegura/NLP4RARE-CM-UC3M.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McDrxOOPEG-D",
        "outputId": "88e1a995-22ea-439f-f81a-be7f86ca23c0"
      },
      "id": "McDrxOOPEG-D",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP4RARE-CM-UC3M'...\n",
            "remote: Enumerating objects: 532, done.\u001b[K\n",
            "remote: Counting objects: 100% (153/153), done.\u001b[K\n",
            "remote: Compressing objects: 100% (95/95), done.\u001b[K\n",
            "remote: Total 532 (delta 84), reused 103 (delta 58), pack-reused 379\u001b[K\n",
            "Receiving objects: 100% (532/532), 7.92 MiB | 7.20 MiB/s, done.\n",
            "Resolving deltas: 100% (89/89), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install smart-open mendelai-brat-parser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fkpx9uYgEGMD",
        "outputId": "049e695c-1186-4bdf-f505-db384e644a15"
      },
      "id": "Fkpx9uYgEGMD",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.8/dist-packages (6.3.0)\n",
            "Collecting mendelai-brat-parser\n",
            "  Downloading mendelai_brat_parser-0.0.11.tar.gz (4.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mendelai-brat-parser\n",
            "  Building wheel for mendelai-brat-parser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mendelai-brat-parser: filename=mendelai_brat_parser-0.0.11-py3-none-any.whl size=4944 sha256=e1e9f2830d757e7a9a6f489c8b7b5da4ea80eb5b2f8dcf095194bd94094da8cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/a7/ff/138853d8196095fec56e0a97779a96d754b98f169c063beca3\n",
            "Successfully built mendelai-brat-parser\n",
            "Installing collected packages: mendelai-brat-parser\n",
            "Successfully installed mendelai-brat-parser-0.0.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/NLP4RARE-CM-UC3M/corpus && unzip train.zip"
      ],
      "metadata": {
        "id": "b6C1CASaGpRl"
      },
      "id": "b6C1CASaGpRl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base_path = '/content/NLP4RARE-CM-UC3M/corpus/train/'\n",
        "\n",
        "all_files = [ fi for fi in os.listdir(base_path) if fi.endswith(\".ann\") or fi.endswith(\".txt\") ]\n",
        "ann_files = [ fi for fi in all_files if not fi.endswith(\".ann\") ]\n",
        "txt_files = [ fi for fi in all_files if not fi.endswith(\".txt\") ]\n",
        "\n",
        "len(all_files) == len(ann_files)+len(txt_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36EgqsBrEFaQ",
        "outputId": "1978eef3-1293-47c4-839c-7e05e77a3cd9"
      },
      "id": "36EgqsBrEFaQ",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from brat_parser import get_entities_relations_attributes_groups\n",
        "\n",
        "def read_corpus(filename):\n",
        "    sentence = {}\n",
        "    txt = open(base_path+filename+'.txt')\n",
        "    sentence['text'] = txt.read()\n",
        "    entities, _, _, _ = get_entities_relations_attributes_groups(base_path+filename+'.ann')\n",
        "    labels = []\n",
        "    for k in entities.keys():\n",
        "        if entities.get(k).text == sentence['text'][entities.get(k).span[0][0]:entities.get(k).span[0][1]]:\n",
        "            labels.append([entities.get(k).span[0][0], entities.get(k).span[0][1], entities.get(k).type])\n",
        "    sentence['label'] = labels\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "0wkaP_KzEFX6"
      },
      "id": "0wkaP_KzEFX6",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotated = []\n",
        "for idx,file in enumerate(ann_files):\n",
        "    annotated.append(read_corpus(file.split(\".\")[0]))"
      ],
      "metadata": {
        "id": "h7WnjSZwEFVb"
      },
      "id": "h7WnjSZwEFVb",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = annotated"
      ],
      "metadata": {
        "id": "d3bLogbWG_yz"
      },
      "id": "d3bLogbWG_yz",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess to feed model"
      ],
      "metadata": {
        "id": "LxUzXSh-zIab"
      },
      "id": "LxUzXSh-zIab"
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[0]"
      ],
      "metadata": {
        "id": "A9Ryz9dlwVT-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9707043d-4957-40f1-a2bb-f83a669cd83a"
      },
      "id": "A9Ryz9dlwVT-",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'Tongue cancers are oral cancers that are differentiated by their location in the mouth and on the tongue. If the cancer is on the forward portion of the tongue, it is known as a squamous cell cancer of the oral tongue. If the cancer is located towards the rear third of the tongue, it is known as a squamous cell cancer at the base of the tongue. Tongue cancer is most common in men over age 60.  It is rare in people, particularly women, under age 40. Examination of a sample of tissue from the site of the suspected cancer by a qualified pathologist is the key to diagnosis. MRI and/or CAT scans may be ordered to determine the location and size of the growth. This examination will also determine the stage of the disorder (how advanced it may be), which in turn, will help determine the method and pace of treatment.\\n',\n",
              " 'label': [[0, 14, 'RAREDISEASE'],\n",
              "  [19, 31, 'DISEASE'],\n",
              "  [113, 119, 'DISEASE'],\n",
              "  [178, 217, 'RAREDISEASE'],\n",
              "  [226, 232, 'DISEASE'],\n",
              "  [299, 345, 'RAREDISEASE'],\n",
              "  [347, 360, 'RAREDISEASE'],\n",
              "  [397, 399, 'ANAPHOR'],\n",
              "  [518, 524, 'DISEASE'],\n",
              "  [713, 725, 'ANAPHOR']]}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4866aa39",
      "metadata": {
        "id": "4866aa39"
      },
      "outputs": [],
      "source": [
        "# Collect all entites into on array\n",
        "all_entities = []\n",
        "for cor in corpus:\n",
        "    entities = []\n",
        "    for e in cor['label']:\n",
        "        build_entity = ''\n",
        "        for i in range(e[0], e[1]):\n",
        "            build_entity += cor['text'][i]\n",
        "        entities.append(build_entity)\n",
        "    all_entities.append(entities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7b3450a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b3450a1",
        "outputId": "ab509fa4-4ea9-42c9-b2a9-ef07770a7474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity Count:  9751\n",
            "Word Count:  115523\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "for en in all_entities:\n",
        "    count += len(en)\n",
        "print('Entity Count: ',count)\n",
        "\n",
        "words = 0\n",
        "for w in corpus:\n",
        "    words += len(w['text'].split(' '))\n",
        "print('Word Count: ', words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "18a066e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18a066e7",
        "outputId": "261ed9ec-4656-470b-eb35-7b17fa22dc94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7408"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "sentences = []\n",
        "for doc in corpus:\n",
        "    for s in doc['text'].split('.'):\n",
        "        sentences.append(s)\n",
        "len(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRu1mXBMrxGy",
        "outputId": "1dc135e4-bf50-48bf-cf4c-6e7fc2dfaab3"
      },
      "id": "eRu1mXBMrxGy",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Get a list of English stop words\n",
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "id": "V_I74Z4mr111"
      },
      "id": "V_I74Z4mr111",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Format documents according required input params"
      ],
      "metadata": {
        "id": "geA45WRnplmW"
      },
      "id": "geA45WRnplmW"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "9ac716dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ac716dd",
        "outputId": "efd98785-51b5-4ca3-ab0f-8fe083261e84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping...88\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "useless_chars = ['(', ')', ':', '\\n\\n', '\\n', '[', ']']\n",
        "concat_sent_cfg = []\n",
        "annotated_docs = []\n",
        "\n",
        "# Select one item from the '''corpus'''\n",
        "for idx,document in enumerate(corpus):\n",
        "    sentences_cfg = []\n",
        "    # Get an array of entities with starting and ending indexes as '''entities''' and '''text'''\n",
        "    curr_entities = document['label']\n",
        "    original_text = document['text']\n",
        "    for uc in useless_chars:\n",
        "        original_text = original_text.replace(uc, ' ')\n",
        "    # Split the text from '''.''' and add the start and ending values from the original text\n",
        "    for curr_sent in original_text.split('.'):\n",
        "        if len(curr_sent) > 5:\n",
        "          try:\n",
        "            for sen in re.finditer(curr_sent, original_text):\n",
        "              sentences_cfg.append({\n",
        "                  'text': curr_sent,\n",
        "                  'start': sen.start(),\n",
        "                  'end': sen.end()\n",
        "              })\n",
        "          except:\n",
        "            print('Skipping...'+ str(idx))\n",
        "    # Check whether the tagged entity include in this sentence or not\n",
        "    for new_sent in sentences_cfg:\n",
        "        curr_labels = []\n",
        "        for en in curr_entities:\n",
        "            if (new_sent['start'] <= en[0] <= new_sent['end']) and (new_sent['start'] <= en[1] <= new_sent['end']):\n",
        "                # add label entities\n",
        "                curr_entity = ''.join(original_text[c] for c in range(en[0], en[1]))\n",
        "                for ent in re.finditer(curr_entity, new_sent['text']):\n",
        "                    curr_labels.append([ent.start(), ent.end(), en[2]]) # start index, end index, type\n",
        "            # Add annotate sentences to annotate_sentences array\n",
        "        annotated_docs.append({\n",
        "            'text': new_sent['text'],\n",
        "            'labels': curr_labels\n",
        "        })\n",
        "    # Append all sentences to concat_sent_cfg\n",
        "    concat_sent_cfg += sentences_cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "847035e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "847035e9",
        "outputId": "f45b03d8-9334-4414-8d16-5addceee8404"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6559"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "len(annotated_docs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Before pass through the POS tagger, we need to remove stop words\n",
        "stop_word = ['and', 'the', 'of', 'a', 'in', 'often', 'or', 'to', 'their', 'more', 'than', 'usual', 'for', 'into', 'test']"
      ],
      "metadata": {
        "id": "aRE8CWyYpPDd"
      },
      "id": "aRE8CWyYpPDd",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "2c088800",
      "metadata": {
        "id": "2c088800"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  {\n",
        "    'text': 'Tick fever is the general name given to a group of diseases that are all caused by a tick bite', \n",
        "    'labels': [[0, 10, 'DISEASE']]\n",
        "  }\n",
        "'''\n",
        "def format_corpus(data, idx):\n",
        "    ann_tup = []\n",
        "    curr_sen_en = [[''.join(data['text'][c] for c in range(en[0], en[1])), en[2]] for en in data['labels']]\n",
        "    for word in data['text'].split(' '):\n",
        "        en_type = 'OTHER'\n",
        "        for cse in curr_sen_en:\n",
        "            for sw in cse[0].split(' '):\n",
        "                if sw == word:\n",
        "                    en_type = cse[1]\n",
        "        if str(word).strip() != '' and not word in stop_word:\n",
        "            ann_tup.append((word, en_type))\n",
        "    return ann_tup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c27922b4",
      "metadata": {
        "id": "c27922b4"
      },
      "source": [
        "## Train POS using Custom Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "6afd46c2",
      "metadata": {
        "id": "6afd46c2"
      },
      "outputs": [],
      "source": [
        "word_ann_docs = []\n",
        "for idx,doc in enumerate(annotated_docs):\n",
        "    word_ann_docs += format_corpus(doc, idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "6faedefc",
      "metadata": {
        "id": "6faedefc"
      },
      "outputs": [],
      "source": [
        "# Helper functions to define the properties of the model\n",
        "def features(sentence, index):\n",
        "    return {\n",
        "        'word': sentence[index],\n",
        "        'is_first': index == 0,\n",
        "        'is_last': index == len(sentence) - 1,\n",
        "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
        "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
        "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
        "        'prefix-1': sentence[index][0],\n",
        "        'prefix-2': sentence[index][:2],\n",
        "        'prefix-3': sentence[index][:3],\n",
        "        'suffix-1': sentence[index][-1],\n",
        "        'suffix-2': sentence[index][-2:],\n",
        "        'suffix-3': sentence[index][-3:],\n",
        "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
        "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
        "        'has_hyphen': '-' in sentence[index],\n",
        "        'is_numeric': sentence[index].isdigit(),\n",
        "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
        "    }\n",
        "\n",
        "def untag(tagged_sentence):\n",
        "    return [w for w, t in tagged_sentence]\n",
        "\n",
        "def transform_to_dataset(tagged_sentences):\n",
        "    X, y = [], []\n",
        " \n",
        "    for tagged in tagged_sentences:\n",
        "        for index in range(len(tagged)):\n",
        "            X.append(features(untag(tagged), index))\n",
        "            y.append(tagged[index][1])\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "36eca5b0",
      "metadata": {
        "id": "36eca5b0"
      },
      "outputs": [],
      "source": [
        "# Setup dataset as [('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ',')]\n",
        "tagged_sentences = [format_corpus(sentence, idx) for idx,sentence in enumerate(annotated_docs)]\n",
        "\n",
        "# Split the dataset for training and testing\n",
        "cutoff = int(.75 * len(tagged_sentences))\n",
        "training_sentences = tagged_sentences[:cutoff]\n",
        "test_sentences = tagged_sentences[cutoff:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "7d87cfc9",
      "metadata": {
        "id": "7d87cfc9"
      },
      "outputs": [],
      "source": [
        "X, y = transform_to_dataset(training_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "4719c759",
      "metadata": {
        "id": "4719c759",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a58e17e3-c944-4c08-8b1a-a88d8e525100"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer', DictVectorizer(sparse=False)),\n",
              "                ('classifier', DecisionTreeClassifier(criterion='entropy'))])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# Train the model using the decision tree classifier to idetify the named entities\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        " \n",
        "clf = Pipeline([\n",
        "    ('vectorizer', DictVectorizer(sparse=False)),\n",
        "    ('classifier', DecisionTreeClassifier(criterion='entropy'))\n",
        "])\n",
        " \n",
        "clf.fit(X[:10000], y[:10000])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model as pickel file\n",
        "# sci-kit learn documentation suggest to save the model as .joblib instead of .pickle\n",
        "import joblib\n",
        "\n",
        "joblib.dump(clf, 'pos-tagger-benchmark.joblib')"
      ],
      "metadata": {
        "id": "S-YbAYZwtKea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8595455-05c0-470b-a369-6f6cbcd07315"
      },
      "id": "S-YbAYZwtKea",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pos-tagger-benchmark.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tryout implementation"
      ],
      "metadata": {
        "id": "o9UmTNqgqIdo"
      },
      "id": "o9UmTNqgqIdo"
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "bzX4YW6jutM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd28195f-bf32-4979-bdee-ce65c485e50e"
      },
      "id": "bzX4YW6jutM-",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "8873dd99",
      "metadata": {
        "id": "8873dd99"
      },
      "outputs": [],
      "source": [
        "# Preprocessing (remove stop words)\n",
        "user_input = \"This model finds disease names such as Cholera, Cancer or COVID\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = nltk.word_tokenize(user_input)\n",
        "\n",
        "# Remove stop words, (stopwords are getting fromt the NLTK library)\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "# Filtered tokens as sentence\n",
        "filtered_user_input = \" \".join(filtered_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "pos_tagger = joblib.load(\"pos-tagger-benchmark.joblib\")"
      ],
      "metadata": {
        "id": "SbX9EQPeunh-"
      },
      "id": "SbX9EQPeunh-",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_user_input"
      ],
      "metadata": {
        "id": "T1hg2jcqu98r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f1f8cb57-975d-4258-e829-8b0a46b97d67"
      },
      "id": "T1hg2jcqu98r",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'model finds disease names Cholera , Cancer COVID'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "a696815c",
      "metadata": {
        "id": "a696815c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c010eaa9-8e7a-4aaa-9521-f246a866f86c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'word': 'model', 'is_first': True, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'm', 'prefix-2': 'mo', 'prefix-3': 'mod', 'suffix-1': 'l', 'suffix-2': 'el', 'suffix-3': 'del', 'prev_word': '', 'next_word': 'finds', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'finds', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'f', 'prefix-2': 'fi', 'prefix-3': 'fin', 'suffix-1': 's', 'suffix-2': 'ds', 'suffix-3': 'nds', 'prev_word': 'model', 'next_word': 'disease', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'disease', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'd', 'prefix-2': 'di', 'prefix-3': 'dis', 'suffix-1': 'e', 'suffix-2': 'se', 'suffix-3': 'ase', 'prev_word': 'finds', 'next_word': 'names', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'names', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'n', 'prefix-2': 'na', 'prefix-3': 'nam', 'suffix-1': 's', 'suffix-2': 'es', 'suffix-3': 'mes', 'prev_word': 'disease', 'next_word': 'Cholera', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'Cholera', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'prefix-1': 'C', 'prefix-2': 'Ch', 'prefix-3': 'Cho', 'suffix-1': 'a', 'suffix-2': 'ra', 'suffix-3': 'era', 'prev_word': 'names', 'next_word': ',', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': ',', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': True, 'prefix-1': ',', 'prefix-2': ',', 'prefix-3': ',', 'suffix-1': ',', 'suffix-2': ',', 'suffix-3': ',', 'prev_word': 'Cholera', 'next_word': 'Cancer', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'Cancer', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'prefix-1': 'C', 'prefix-2': 'Ca', 'prefix-3': 'Can', 'suffix-1': 'r', 'suffix-2': 'er', 'suffix-3': 'cer', 'prev_word': ',', 'next_word': 'COVID', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'COVID', 'is_first': False, 'is_last': True, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'C', 'prefix-2': 'CO', 'prefix-3': 'COV', 'suffix-1': 'D', 'suffix-2': 'ID', 'suffix-3': 'VID', 'prev_word': 'Cancer', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': True}]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('model', 'OTHER'),\n",
              " ('finds', 'OTHER'),\n",
              " ('disease', 'DISEASE'),\n",
              " ('names', 'OTHER'),\n",
              " ('Cholera', 'OTHER'),\n",
              " (',', 'OTHER'),\n",
              " ('Cancer', 'OTHER'),\n",
              " ('COVID', 'DISEASE')]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "from nltk import word_tokenize, pos_tag\n",
        "\n",
        "def pos_tag(sentence):\n",
        "  print([features(sentence, index) for index in range(len(sentence))])\n",
        "  tags = pos_tagger.predict([features(sentence, index) for index in range(len(sentence))])\n",
        "  return zip(sentence, tags)\n",
        "\n",
        "list(pos_tag(word_tokenize(filtered_user_input)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "Vs-s1FNoGtb9"
      },
      "id": "Vs-s1FNoGtb9"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "a1b9b614",
      "metadata": {
        "id": "a1b9b614",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ccd4d69-8385-4528-e615-9558447cc890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8435126101488909\n"
          ]
        }
      ],
      "source": [
        "X_test, y_test = transform_to_dataset(test_sentences)\n",
        "print(\"Accuracy:\", clf.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = y_test # Test data\n",
        "y_pred = [pos_tagger.predict(word)[0] for word in X_test]\n",
        "target_names = list(set(y_test))"
      ],
      "metadata": {
        "id": "JoSIA-VXGzkt"
      },
      "id": "JoSIA-VXGzkt",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(target_names)"
      ],
      "metadata": {
        "id": "cEKy4SPU1rZ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24060f63-e945-40ca-86aa-bab68624ed15"
      },
      "id": "cEKy4SPU1rZ1",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['SYMPTOM', 'SKINRAREDISEASE', 'SIGN', 'ANAPHOR', 'RAREDISEASE', 'DISEASE', 'OTHER']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform one hot encoding\n",
        "\n",
        "from sklearn import preprocessing\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "encoder.fit(target_names)\n",
        "\n",
        "actual = encoder.transform(y_true)\n",
        "predict = encoder.transform(y_pred)"
      ],
      "metadata": {
        "id": "QlvBPC0wbx9h"
      },
      "id": "QlvBPC0wbx9h",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(actual, predict, target_names=target_names))"
      ],
      "metadata": {
        "id": "2WGqs0E4AYQ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5dcf2fc-053f-4e6f-f8ed-51dba9311d08"
      },
      "id": "2WGqs0E4AYQ4",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "        SYMPTOM       0.81      0.75      0.78       413\n",
            "SKINRAREDISEASE       0.36      0.30      0.33       529\n",
            "           SIGN       0.89      0.96      0.92     18518\n",
            "        ANAPHOR       0.61      0.41      0.49      1474\n",
            "    RAREDISEASE       0.46      0.29      0.35      1826\n",
            "        DISEASE       0.17      0.05      0.08       144\n",
            "          OTHER       0.93      0.21      0.34       133\n",
            "\n",
            "       accuracy                           0.84     23037\n",
            "      macro avg       0.61      0.43      0.47     23037\n",
            "   weighted avg       0.82      0.84      0.83     23037\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}