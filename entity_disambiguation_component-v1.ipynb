{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBlPtNmsDGgc"
      },
      "source": [
        "## Install package and download required data files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOmdtzKADFVP",
        "outputId": "adbe909c-4220-4b94-9c5d-ee6b7c1ceead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-30 04:47:32--  https://raw.githubusercontent.com/viraj-lakshitha/animal-disease-symptom-ontology/develop/ADSOv1.0.3.owl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68571 (67K) [text/plain]\n",
            "Saving to: ‘ADSOv1.0.3.owl.1’\n",
            "\n",
            "ADSOv1.0.3.owl.1    100%[===================>]  66.96K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2023-01-30 04:47:32 (38.2 MB/s) - ‘ADSOv1.0.3.owl.1’ saved [68571/68571]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download ADSO ontology file\n",
        "!wget \"https://raw.githubusercontent.com/viraj-lakshitha/animal-disease-symptom-ontology/develop/ADSOv1.0.3.owl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9s81WWgDe0A",
        "outputId": "aa1c5757-4e20-4b9f-cdf0-acc2aaad882a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.8/dist-packages (6.2.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.8/dist-packages (2.2.2)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.8/dist-packages (0.12.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from rdflib) (3.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from rdflib) (57.4.0)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.8/dist-packages (from rdflib) (0.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.11)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.14.1+cu116)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.1.97)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub) (4.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from isodate->rdflib) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install rdflib spacy sentence-transformers huggingface-hub transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4v4SEn8D0RA"
      },
      "source": [
        "## Load required pre-trained models\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SI807hKlEbDy",
        "outputId": "0f1c7e37-aadc-4c46-a9f1-0c8fac45b4d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Download Wordnet dependencies\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Get a list of English stop words\n",
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "id": "vEOIIWsDS-w-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "7fa1aaeae5a14471b61afe3b7b8f500f",
            "2adce70529e443ef81130c1bf7c249d6",
            "c6d848adddd946509fe893613faa2c2a",
            "2c381ea14db6466cbc5e33f27e315665",
            "6bc10bd46dd94ac282a50b661655d266",
            "7176f289a4cc4c72a6f72be6f143d5ac",
            "57a236e1360a4cb78f7c6ce637dd8f0f",
            "0c26eae418f7421b8aaa92d66dc1ae54",
            "2be4de0d10414561b02e64fdbb313089",
            "6b5077b19162454699720d0e1bade77d",
            "19cb0a1f92e04f1f9ef961340db5807b"
          ]
        },
        "id": "InmZXdRODnc5",
        "outputId": "7a883a63-a514-4003-fca8-891913eefcd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "config.json not found in HuggingFace Hub\n",
            "WARNING:huggingface_hub.hub_mixin:config.json not found in HuggingFace Hub\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7fa1aaeae5a14471b61afe3b7b8f500f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "from huggingface_hub import from_pretrained_keras\n",
        "import numpy as np\n",
        "import transformers\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "\n",
        "# POS tagger\n",
        "pos_tagger = joblib.load(\"/content/pos-tagger.joblib\")\n",
        "\n",
        "# Text similarity model\n",
        "tsm = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2');\n",
        "\n",
        "# Semantic similarity model\n",
        "ssm = from_pretrained_keras(\"keras-io/bert-semantic-similarity\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions to define the properties of the pos-tagger model\n",
        "def features(sentence, index):\n",
        "    return {\n",
        "        'word': sentence[index],\n",
        "        'is_first': index == 0,\n",
        "        'is_last': index == len(sentence) - 1,\n",
        "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
        "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
        "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
        "        'prefix-1': sentence[index][0],\n",
        "        'prefix-2': sentence[index][:2],\n",
        "        'prefix-3': sentence[index][:3],\n",
        "        'suffix-1': sentence[index][-1],\n",
        "        'suffix-2': sentence[index][-2:],\n",
        "        'suffix-3': sentence[index][-3:],\n",
        "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
        "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
        "        'has_hyphen': '-' in sentence[index],\n",
        "        'is_numeric': sentence[index].isdigit(),\n",
        "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
        "    }"
      ],
      "metadata": {
        "id": "AXsZfqdwWJsP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWRSUfHdDYGl"
      },
      "source": [
        "## Helper Utils\n",
        "\n",
        "This block contains functions/classes/enums that use throught out the entity disambiguation component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VjA8BQv4Dda7"
      },
      "outputs": [],
      "source": [
        "# Help utils for semantic similarity model\n",
        "class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n",
        "    \"\"\"Generates batches of data.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        sentence_pairs,\n",
        "        labels,\n",
        "        batch_size=32,\n",
        "        shuffle=True,\n",
        "        include_targets=True,\n",
        "    ):\n",
        "        self.sentence_pairs = sentence_pairs\n",
        "        self.labels = labels\n",
        "        self.shuffle = shuffle\n",
        "        self.batch_size = batch_size\n",
        "        self.include_targets = include_targets\n",
        "        # Load our BERT Tokenizer to encode the text.\n",
        "        # We will use base-base-uncased pretrained model.\n",
        "        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n",
        "            \"bert-base-uncased\", do_lower_case=True\n",
        "        )\n",
        "        self.indexes = np.arange(len(self.sentence_pairs))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Denotes the number of batches per epoch.\n",
        "        return len(self.sentence_pairs) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Retrieves the batch of index.\n",
        "        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        sentence_pairs = self.sentence_pairs[indexes]\n",
        "\n",
        "        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n",
        "        # encoded together and separated by [SEP] token.\n",
        "        encoded = self.tokenizer.batch_encode_plus(\n",
        "            sentence_pairs.tolist(),\n",
        "            add_special_tokens=True,\n",
        "            max_length=128,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=True,\n",
        "            pad_to_max_length=True,\n",
        "            return_tensors=\"tf\",\n",
        "        )\n",
        "\n",
        "        # Convert batch of encoded features to numpy array.\n",
        "        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
        "\n",
        "        # Set to true if data generator is used for training/validation.\n",
        "        if self.include_targets:\n",
        "            labels = np.array(self.labels[indexes], dtype=\"int32\")\n",
        "            return [input_ids, attention_masks, token_type_ids], labels\n",
        "        else:\n",
        "            return [input_ids, attention_masks, token_type_ids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EGU_PZl9YP4C"
      },
      "outputs": [],
      "source": [
        "labels = [\"negative_similarity\", \"positive_similarity\", \"neutral\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhUv5WvtFmRG"
      },
      "source": [
        "## Data loader component\n",
        "\n",
        "In here we are going to retreive the all type of entities/nodes from the ontology knowledgebase and store in seperate arrays.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAVqHRX9Fl3G",
        "outputId": "12d8bcb7-cbf4-435e-ec6d-72431fff0eab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Graph identifier=N39af3b12f00542aa8908ea8424a9c617 (<class 'rdflib.graph.Graph'>)>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Load downloaded ontology graph\n",
        "from rdflib import Graph, Namespace, Literal, RDF, URIRef\n",
        "\n",
        "g = Graph()\n",
        "g.parse(\"/content/ADSOv1.0.3.owl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Dk_hfKFRF24-"
      },
      "outputs": [],
      "source": [
        "# Get the URIRef for given keyword\n",
        "def get_ref(keyword):\n",
        "    return URIRef(\"https://ontology.drpawspaw.com/\"+keyword)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wKdr7P0VF222"
      },
      "outputs": [],
      "source": [
        "# Get the entity 'text' from the URIRef\n",
        "def get_text_from_uri(uri):\n",
        "    for s,p,o in g:\n",
        "        if s == uri and p == URIRef(\"https://ontology.drpawspaw.com/text\"):\n",
        "            return o\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_text_from_uri(URIRef('https://ontology.drpawspaw.com/ADSO0000000083')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKBVN22L472b",
        "outputId": "0bfa6a34-242d-4b2f-deba-a36eb31cf230"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distempter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list of symptoms from disease URI\n",
        "def get_symptoms_from_disease_uri(uri):\n",
        "  symptoms = []\n",
        "  for s,p,o in g:\n",
        "    if s == uri and p == URIRef(\"https://ontology.drpawspaw.com/hasSymptom\"):\n",
        "      symptoms.append(o.toPython())\n",
        "  return symptoms"
      ],
      "metadata": {
        "id": "5QAAdeY47KNE"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_symptoms_from_disease_uri(URIRef('https://ontology.drpawspaw.com/ADSO0000000083'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ok6CoQJ27hEX",
        "outputId": "70f3219f-ea46-4245-8571-abd8a7df0dd9"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://ontology.drpawspaw.com/ADSO0000000027',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000034',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000031',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000033',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000036',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000030',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000040',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000026',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000017',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000037',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000032',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000005',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000038',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000023',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000039',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000008',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000093',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000009',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000007',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000028']"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get URI from text\n",
        "def get_uri_from_text(text):\n",
        "  for s, p, o in g:\n",
        "    if p == URIRef(\"https://ontology.drpawspaw.com/text\"):\n",
        "      if o.toPython().lower() == text.lower():\n",
        "        return s\n",
        "  return None"
      ],
      "metadata": {
        "id": "bEy_tsWC34mY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_uri_from_text(\"fever\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "0cD6ogjF4LPV",
        "outputId": "b8b4f4aa-84cf-47a6-a35c-7a4d03cc88b9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "rdflib.term.URIRef('https://ontology.drpawspaw.com/ADSO0000000005')"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47Zcfs0zF20V",
        "outputId": "1b145818-a73a-46f0-9f4b-93b28c193443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'rdflib.term.URIRef'> https://ontology.drpawspaw.com/ADSO0000000110\n"
          ]
        }
      ],
      "source": [
        "# Collect named entities from ontology\n",
        "diseases = []\n",
        "symptoms = []\n",
        "synonyms = []\n",
        "\n",
        "for s,p,o in g:\n",
        "    if p == get_ref(\"hasDisease\"):\n",
        "        try:\n",
        "            dis = get_text_from_uri(o).toPython()\n",
        "            diseases.append(dis)\n",
        "        except:\n",
        "            print(type(o), o)\n",
        "    if p == get_ref(\"hasSymptom\"):\n",
        "        try:\n",
        "            sym = get_text_from_uri(o).toPython()\n",
        "            symptoms.append(sym)\n",
        "        except:\n",
        "            print(type(o), o)\n",
        "    if p == get_ref(\"hasSynonym\"):\n",
        "        try:\n",
        "            syn = get_text_from_uri(o).toPython()\n",
        "            synonyms.append(syn)\n",
        "        except:\n",
        "            print(type(o), o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "FbvkQWIVF2O2"
      },
      "outputs": [],
      "source": [
        "# List of symptoms and their synonyms\n",
        "symp_syn = []\n",
        "\n",
        "# Check the keyword already exist or not in collection\n",
        "def is_exist_ss(keyword):\n",
        "    for _,k,_ in symp_syn:\n",
        "        if k == keyword:\n",
        "            return True\n",
        "            \n",
        "# Apped all the symptoms and their synonyms\n",
        "for s,p,o in g: # subject, predicate, object\n",
        "    # filter synonyms\n",
        "    if p == get_ref(\"hasSymptom\"):\n",
        "        try:\n",
        "            x = get_text_from_uri(o).toPython()\n",
        "        except:\n",
        "            continue\n",
        "        curr_symp_syn = []\n",
        "        for s1, p1, o1 in g:\n",
        "            # filter synonym for above \"o\" entity\n",
        "            if s1 == o and p1 == get_ref(\"hasSynonym\"):\n",
        "                try:\n",
        "                    y = get_text_from_uri(o1).toPython()\n",
        "                    curr_symp_syn.append(y)                    \n",
        "                except:\n",
        "                    continue\n",
        "        # validate to add only one entry\n",
        "        if not is_exist_ss(x):\n",
        "            try:\n",
        "                idx = o.toPython()\n",
        "                symp_syn.append((idx, x, curr_symp_syn))\n",
        "            except:\n",
        "                continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxh0vvqJFlvA",
        "outputId": "f40e268c-23df-4a17-a706-f7319026d909"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('https://ontology.drpawspaw.com/ADSO0000000027', 'Eye Discharge', []),\n",
              " ('https://ontology.drpawspaw.com/ADSO0000000034', 'Muscle Twitching', []),\n",
              " ('https://ontology.drpawspaw.com/ADSO0000000016', 'Blood Diarrhea', []),\n",
              " ('https://ontology.drpawspaw.com/ADSO0000000029',\n",
              "  'Scratching Ears',\n",
              "  ['Crusting in the Ears']),\n",
              " ('https://ontology.drpawspaw.com/ADSO0000000042', 'Running Nose', [])]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "symp_syn[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmZzuzcRHDSV",
        "outputId": "099a0782-917e-4ded-f680-1f216e95dd90"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "len(symp_syn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfGSSJUBGvyo"
      },
      "source": [
        "## Text similarity\n",
        "\n",
        "In here we are going to calculate the similarity of the words using pretrained model from Hugging Face => Text Similarity - To get the similar words for identified named entities (https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2, https://huggingface.co/tasks/sentence-similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "OQ42zIs8O7B_"
      },
      "outputs": [],
      "source": [
        "# Generate synonyms for the existing symptoms and update the list of synonyms\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def generate_synonyms(uri, keyword, syns):\n",
        "    for synonym in wordnet.synsets(keyword):\n",
        "        for item in synonym.lemmas():\n",
        "            if keyword != synonym.name() and len(synonym.lemma_names()) > 1:\n",
        "                syns.append(item.name())\n",
        "    return (uri, keyword, syns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "rSjE0NnrRzPj"
      },
      "outputs": [],
      "source": [
        "# for idx,(uri,keyword,syns) in enumerate(symp_syn):\n",
        "#   symp_syn[idx] = generate_synonyms(uri,keyword,syns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQU9S-PdSL3K",
        "outputId": "47339085-a8cb-453c-e502-3adb79a3410c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79\n",
            "[('https://ontology.drpawspaw.com/ADSO0000000027', 'Eye Discharge', []), ('https://ontology.drpawspaw.com/ADSO0000000034', 'Muscle Twitching', []), ('https://ontology.drpawspaw.com/ADSO0000000016', 'Blood Diarrhea', []), ('https://ontology.drpawspaw.com/ADSO0000000029', 'Scratching Ears', ['Crusting in the Ears']), ('https://ontology.drpawspaw.com/ADSO0000000042', 'Running Nose', [])]\n"
          ]
        }
      ],
      "source": [
        "print(len(symp_syn))\n",
        "print(symp_syn[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "MpgG4VkkOjRE"
      },
      "outputs": [],
      "source": [
        "# Exapand the all entities\n",
        "expanded_symp = []\n",
        "\n",
        "def is_exist_es(keyword):\n",
        "    for idx,word in expanded_symp:\n",
        "        if keyword == word:\n",
        "            return True\n",
        "\n",
        "for idx,word,syns in symp_syn:\n",
        "    for s in syns:\n",
        "        if not is_exist_es(s):\n",
        "            expanded_symp.append((idx, s))\n",
        "    if not is_exist_es(word):\n",
        "        expanded_symp.append((idx, word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G5MdZKBPLuS",
        "outputId": "f70a36ac-c10b-4476-a9c3-67cfc5d49b54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101\n",
            "[('https://ontology.drpawspaw.com/ADSO0000000027', 'Eye Discharge'), ('https://ontology.drpawspaw.com/ADSO0000000034', 'Muscle Twitching'), ('https://ontology.drpawspaw.com/ADSO0000000016', 'Blood Diarrhea'), ('https://ontology.drpawspaw.com/ADSO0000000029', 'Crusting in the Ears'), ('https://ontology.drpawspaw.com/ADSO0000000029', 'Scratching Ears')]\n"
          ]
        }
      ],
      "source": [
        "print(len(expanded_symp))\n",
        "print(expanded_symp[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "g_xdYqyvG_hI"
      },
      "outputs": [],
      "source": [
        "# Calculate the similariity\n",
        "# Here \"tsm\" means the text-similarity-model, that we loaded in \"Load required pre-trained models\"\n",
        "# \"get_similarity\" accepts the URI, identified named entity, entity from ontology\n",
        "def get_similarity(idx, word, ne): # Return a tuple, contain URI, node, similarity_score\n",
        "    embedding_1 = tsm.encode(ne, convert_to_tensor=True)\n",
        "    embedding_2 = tsm.encode(word, convert_to_tensor=True)\n",
        "    return (idx, word, util.pytorch_cos_sim(embedding_1, embedding_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "zRNcOcRpS4Bx"
      },
      "outputs": [],
      "source": [
        "# At the current implementation, we only get first five entities\n",
        "# \"entity\" - identified named entity, \"nodes\" - named entities in the onotology (in format of [(idx, text)])\n",
        "def get_most_similarity_entities(entity, nodes):\n",
        "  scores = map(lambda e: get_similarity(e[0], e[1], entity), nodes)\n",
        "  return sorted(list(scores), key=lambda x:x[2], reverse=True)[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2_OOmWaRJS7",
        "outputId": "b3d3472e-82f2-46de-8bc1-e23ed7c599bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('https://ontology.drpawspaw.com/ADSO0000000005',\n",
              "  'Fever',\n",
              "  tensor([[1.]], device='cuda:0')),\n",
              " ('https://ontology.drpawspaw.com/ADSO0000000041',\n",
              "  'High Fever',\n",
              "  tensor([[0.8284]], device='cuda:0')),\n",
              " ('https://ontology.drpawspaw.com/ADSO0000000032',\n",
              "  'Pneumonia',\n",
              "  tensor([[0.4948]], device='cuda:0')),\n",
              " ('https://ontology.drpawspaw.com/ADSO0000000041',\n",
              "  'High Temperature',\n",
              "  tensor([[0.4413]], device='cuda:0')),\n",
              " ('https://ontology.drpawspaw.com/ADSO0000000008',\n",
              "  'Vomiting',\n",
              "  tensor([[0.4372]], device='cuda:0'))]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# Tryout the implementation\n",
        "identified_ne = [\"fever\", \"lethargy\"]\n",
        "\n",
        "# Get the top five entities from the text similarity, accroding to the cosine similarity score\n",
        "get_most_similarity_entities(identified_ne[0], expanded_symp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6ilvSv6XtzP"
      },
      "source": [
        "## Semantic Similarity\n",
        "\n",
        "In here we are going to calculate the semantic similarity of the words using pretrained model from HuggingFace => Semantic Similarity - To get the similarity in context for the ranked candidates (https://huggingface.co/keras-io/bert-semantic-similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "5WPFjjDKX-Z-"
      },
      "outputs": [],
      "source": [
        "# Calculate the semantic similarity of the identified entity and nodes in ontology\n",
        "# \"calculate_semantic_similarity\" functions accpets, the URI, text and identified entity\n",
        "def calculate_semantic_similarity(uri, ctx, entity):\n",
        "    sentence_pairs = np.array([[str(entity), str(ctx)]])\n",
        "    test_data = BertSemanticDataGenerator(\n",
        "        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n",
        "    )\n",
        "    probs = ssm.predict(test_data[0])[0]\n",
        "    labels_probs = {labels[i]: float(probs[i]) for i, _ in enumerate(labels)}\n",
        "    return (uri, ctx, labels_probs[\"positive_similarity\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "aHozNg4DYkaq"
      },
      "outputs": [],
      "source": [
        "# \"get_highest_ctx_similarity\" function return the highest context similarity as tuple (URI, Word, ctx_similarity)\n",
        "def get_highest_ctx_similarity(entity, nodes):\n",
        "  scores = map(lambda e: calculate_semantic_similarity(e[0], e[1], entity), nodes)\n",
        "  return sorted(list(scores), key=lambda x:x[2], reverse=True)[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ndR0sB6YoDd",
        "outputId": "0c8cd952-3ff0-472a-f4ed-b0e975c316c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 71ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 86ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 60ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 61ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('https://ontology.drpawspaw.com/ADSO0000000005',\n",
              "  'Fever',\n",
              "  0.9982507824897766)]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "ine = \"fever\"\n",
        "candidates = get_most_similarity_entities(ine, expanded_symp) # Candidates from text-similarity\n",
        "get_highest_ctx_similarity(ine, candidates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci4b6Q6-IyJX"
      },
      "source": [
        "## Query data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "rYLK3UFeI3Bd"
      },
      "outputs": [],
      "source": [
        "# This function return SPARQL query, that able to get the disease from symptoms\n",
        "def build_query(symptoms):\n",
        "  return \"\"\"\n",
        "    PREFIX adso: <https://ontology.drpawspaw.com/>\n",
        "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
        "    SELECT ?diseaseName\n",
        "    WHERE {{\n",
        "        {sym_query}\n",
        "        ?diseaseUri adso:text ?diseaseName .\n",
        "    }}\n",
        "    \"\"\".format(sym_query=\"\\n\".join(\n",
        "        list(\n",
        "            map(lambda e: \"?diseaseUri adso:hasSymptom adso:{uri} .\".format(uri=e),\n",
        "                list(map(lambda e: e.split(\"/\")[3], symptoms))))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgQxHsybJ2s9",
        "outputId": "821944f7-e1dc-4c6c-c27e-346daf0b1372"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(rdflib.term.Literal('Babesiosis', lang='en'),)\n"
          ]
        }
      ],
      "source": [
        "q = build_query([\"https://ontology.drpawspaw.com/ADSO0000000005\",\n",
        "    \"https://ontology.drpawspaw.com/ADSO0000000006\",\n",
        "    \"https://ontology.drpawspaw.com/ADSO0000000007\"])\n",
        "\n",
        "for output in g.query(q):\n",
        "  print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation tryout"
      ],
      "metadata": {
        "id": "xFVfyOFvSwR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing inputs"
      ],
      "metadata": {
        "id": "hp4vB2bgjsqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing (remove stop words)\n",
        "# user_input = \"My dog has been vomiting has Limb Swelling and has diarrhea\"\n",
        "user_input = \"My dog has been vomiting and has diarrhea \"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = nltk.word_tokenize(user_input)\n",
        "\n",
        "# Remove stop words, (stopwords are getting fromt the NLTK library)\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "# Filtered tokens as sentence\n",
        "filtered_user_input = \" \".join(filtered_tokens)"
      ],
      "metadata": {
        "id": "fpPPSzXlSvse"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_user_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vCtQ5yiJV9-S",
        "outputId": "ceda4bb6-20d5-4759-9891-a2924e1e4e28"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dog vomiting diarrhea'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "def pos_tag(sentence):\n",
        "    tags = pos_tagger.predict([features(sentence, index) for index in range(len(sentence))])\n",
        "    return zip(sentence, tags)\n",
        "\n",
        "annotated_entites = [en for en in list(\n",
        "      pos_tag(word_tokenize(filtered_user_input)) # Filter entities annotate as \"SYMPTOM\"\n",
        "    ) if en[1] == \"SYMPTOM\"]"
      ],
      "metadata": {
        "id": "KCBSszIgV_0U"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotated_entites"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K3wqM0cXyPz",
        "outputId": "03f2d258-cfb5-421a-aa4c-3a53acb8db07"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('vomiting', 'SYMPTOM'), ('diarrhea', 'SYMPTOM')]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing entity linker component"
      ],
      "metadata": {
        "id": "Ob8ZkNGbj10B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "link_entity = []\n",
        "\n",
        "for entity in annotated_entites:\n",
        "  current_entity_candidates = get_most_similarity_entities(entity[0], expanded_symp)\n",
        "  link_entity.append(get_highest_ctx_similarity(entity[0], current_entity_candidates))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrOtqWTLYjKg",
        "outputId": "2ffe7bb9-52ae-4d01-ad2b-15e691acbb96"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 62ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 58ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 57ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 57ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 59ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 56ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 64ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 60ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 58ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 64ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "link_entity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIUkY60Xb2xd",
        "outputId": "33e65a9e-72ed-4e59-be1b-4fb04d3553e0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('https://ontology.drpawspaw.com/ADSO0000000008',\n",
              "   'Vomiting',\n",
              "   0.9984850287437439)],\n",
              " [('https://ontology.drpawspaw.com/ADSO0000000009',\n",
              "   'Diarrhea',\n",
              "   0.9998852014541626)]]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query symptoms to get the disease"
      ],
      "metadata": {
        "id": "3LZ1xnuVj-JH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entities = list(map(lambda x: x[0][0], link_entity))"
      ],
      "metadata": {
        "id": "oyZZnlXhZqLq"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_u6x4P5h2_p",
        "outputId": "c39acfa5-9fb4-456b-c2ef-184ffd0a84ed"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://ontology.drpawspaw.com/ADSO0000000008',\n",
              " 'https://ontology.drpawspaw.com/ADSO0000000009']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "curr_query = build_query(entities)"
      ],
      "metadata": {
        "id": "3h-2S0Y4btmC"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "curr_query"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "JaH0yGv_b7up",
        "outputId": "3ce0e4ec-4154-4907-fef8-4b32f4d95a8d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    PREFIX adso: <https://ontology.drpawspaw.com/>\\n    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\\n    SELECT ?diseaseName\\n    WHERE {\\n        ?diseaseUri adso:hasSymptom adso:ADSO0000000008 .\\n?diseaseUri adso:hasSymptom adso:ADSO0000000009 .\\n        ?diseaseUri adso:text ?diseaseName .\\n    }\\n    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for rq in g.query(curr_query):\n",
        "  print(rq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nLu2YRqZ2F6",
        "outputId": "99cbfbd9-1867-4b01-c8c4-61c32aa139bf"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(rdflib.term.Literal('Babesiosis', lang='en'),)\n",
            "(rdflib.term.Literal('Distempter', lang='en'),)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WORD_JOIN_CHAR = \" \"; # Use to join the words in an array\n",
        "\n",
        "from nltk import word_tokenize\n",
        "\n",
        "def pos_tag(sentence):\n",
        "    tags = pos_tagger.predict([features(sentence, index) for index in range(len(sentence))])\n",
        "    return zip(sentence, tags)\n",
        "\n",
        "def entity_linker(sentence):\n",
        "  etq = [] # Entities to query\n",
        "  annotated_entites = [en for en in list(\n",
        "      pos_tag(word_tokenize(filtered_user_input)) # Filter entities annotate as \"SYMPTOM\"\n",
        "    ) if en[1] == \"SYMPTOM\"]\n",
        "  print(\"Identified named entities => \", annotated_entites)\n",
        "  for e in annotated_entites:\n",
        "    current_entity_candidates = get_most_similarity_entities(e[0], expanded_symp)\n",
        "    print(\"Candidates => \",e, current_entity_candidates)\n",
        "    etq.append(get_highest_ctx_similarity(e, current_entity_candidates))\n",
        "    print(\"Named Entity => \",e, etq)\n",
        "  current_query = build_query(list(map(lambda x: x[0][0], etq)))\n",
        "  return [rq for rq in g.query(current_query)]"
      ],
      "metadata": {
        "id": "Di8KXw6Tw1Cr"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entity_linker(\"My dog has been vomiting has Limb Swelling and has diarrhea\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLn2fQdgxzco",
        "outputId": "20da76ab-2a79-4914-822d-a236a06e7111"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified named entities =>  [('vomiting', 'SYMPTOM'), ('diarrhea', 'SYMPTOM')]\n",
            "Candidates =>  ('vomiting', 'SYMPTOM') [('https://ontology.drpawspaw.com/ADSO0000000008', 'Vomiting', tensor([[1.0000]], device='cuda:0')), ('https://ontology.drpawspaw.com/ADSO0000000057', 'Nausea', tensor([[0.7699]], device='cuda:0')), ('https://ontology.drpawspaw.com/ADSO0000000009', 'Diarrhea', tensor([[0.6183]], device='cuda:0')), ('https://ontology.drpawspaw.com/ADSO0000000066', 'Eating Poison', tensor([[0.5464]], device='cuda:0')), ('https://ontology.drpawspaw.com/ADSO0000000007', 'coughing', tensor([[0.5324]], device='cuda:0'))]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 61ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 91ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 129ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 125ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 110ms/step\n",
            "Named Entity =>  ('vomiting', 'SYMPTOM') [[('https://ontology.drpawspaw.com/ADSO0000000008', 'Vomiting', 0.9960421323776245)]]\n",
            "Candidates =>  ('diarrhea', 'SYMPTOM') [('https://ontology.drpawspaw.com/ADSO0000000009', 'Diarrhea', tensor([[1.]], device='cuda:0')), ('https://ontology.drpawspaw.com/ADSO0000000016', 'Blood Diarrhea', tensor([[0.7598]], device='cuda:0')), ('https://ontology.drpawspaw.com/ADSO0000000008', 'Vomiting', tensor([[0.6183]], device='cuda:0')), ('https://ontology.drpawspaw.com/ADSO0000000057', 'Nausea', tensor([[0.6028]], device='cuda:0')), ('https://ontology.drpawspaw.com/ADSO0000000059', 'Bloated Tummy', tensor([[0.4899]], device='cuda:0'))]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 104ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 96ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 83ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 108ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 100ms/step\n",
            "Named Entity =>  ('diarrhea', 'SYMPTOM') [[('https://ontology.drpawspaw.com/ADSO0000000008', 'Vomiting', 0.9960421323776245)], [('https://ontology.drpawspaw.com/ADSO0000000009', 'Diarrhea', 0.9990774393081665)]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(rdflib.term.Literal('Babesiosis', lang='en')),\n",
              " (rdflib.term.Literal('Distempter', lang='en'))]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain\n",
        "\n",
        "def make_prediction(sentence):\n",
        "  prediction = entity_linker(sentence)\n",
        "  # prediction = [(rdflib.term.Literal('Babesiosis', lang='en'),), (rdflib.term.Literal('Distempter', lang='en'),)]\n",
        "  # If there are multiple predictions from the given symptoms\n",
        "  if len(prediction) > 1:\n",
        "    # Retrieve the URI for above prediction\n",
        "    print(\"prediction\", prediction)\n",
        "    duris = list(map(lambda x: get_uri_from_text(x[0].toPython()).toPython(), prediction))\n",
        "    # Get the all unique symptoms for predicted diseases\n",
        "    symptoms_uris = list(map(lambda x: get_symptoms_from_disease_uri(URIRef(x)), list(duris)))\n",
        "    print(\"symptoms_uris\", symptoms_uris)\n",
        "    # Convert back to symptoms into readable format\n",
        "    symptom_suggestions = list(map(lambda x: get_text_from_uri(URIRef(x)).toPython(), list(chain.from_iterable(symptoms_uris))))\n",
        "    print(\"symptom_suggestions\", symptom_suggestions)\n",
        "    # Return symptoms suggestions for the better prediction\n",
        "    return {\n",
        "      \"request\": sentence,\n",
        "      \"result_type\": \"SUGGESTION\",\n",
        "      \"symptom_suggestions\": set(symptom_suggestions),\n",
        "      \"predicted_disease\": None\n",
        "    }\n",
        "  # Unable to predict\n",
        "  if len(prediction) < 1:\n",
        "    return {\n",
        "      \"request\": sentence,\n",
        "      \"result_type\": \"LIMITATION\",\n",
        "      \"symptom_suggestions\": None,\n",
        "      \"predicted_disease\": None\n",
        "    }\n",
        "  # Return the prediction\n",
        "  result = {\n",
        "      \"request\": sentence,\n",
        "      \"result_type\": \"PREDICTION\",\n",
        "      \"symptom_suggestions\": None,\n",
        "      \"predicted_disease\": prediction[0]\n",
        "  }\n",
        "  return result"
      ],
      "metadata": {
        "id": "U9_eodgk2GLR"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "print(make_prediction('My dog has been vomiting has Limb Swelling and has diarrhea'))\n",
        "print(\"--- Time elaped for single execution: \", time.time()-start_time, \" ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2bXQUnpGW7x",
        "outputId": "16cd39e3-2973-416a-c695-4e218077da93"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified named entities =>  [('vomiting', 'SYMPTOM'), ('diarrhea', 'SYMPTOM')]\n",
            "Candidates =>  ('vomiting', 'SYMPTOM') [('https://ontology.drpawspaw.com/ADSO0000000008', 'Vomiting', tensor([[1.0000]], device='cuda:0')), ('https://ontology.drpawspaw.com/ADSO0000000057', 'Nausea', tensor([[0.7699]], device='cuda:0')), ('https://ontology.drpawspaw.com/ADSO0000000009', 'Diarrhea', tensor([[0.6183]], device='cuda:0')), ('https://ontology.drpawspaw.com/ADSO0000000066', 'Eating Poison', tensor([[0.5464]], device='cuda:0')), ('https://ontology.drpawspaw.com/ADSO0000000007', 'coughing', tensor([[0.5324]], device='cuda:0'))]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 56ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 62ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 60ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 57ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 59ms/step\n",
            "Named Entity =>  ('vomiting', 'SYMPTOM') [[('https://ontology.drpawspaw.com/ADSO0000000008', 'Vomiting', 0.9960421323776245)]]\n",
            "Candidates =>  ('diarrhea', 'SYMPTOM') [('https://ontology.drpawspaw.com/ADSO0000000009', 'Diarrhea', tensor([[1.]], device='cuda:0')), ('https://ontology.drpawspaw.com/ADSO0000000016', 'Blood Diarrhea', tensor([[0.7598]], device='cuda:0')), ('https://ontology.drpawspaw.com/ADSO0000000008', 'Vomiting', tensor([[0.6183]], device='cuda:0')), ('https://ontology.drpawspaw.com/ADSO0000000057', 'Nausea', tensor([[0.6028]], device='cuda:0')), ('https://ontology.drpawspaw.com/ADSO0000000059', 'Bloated Tummy', tensor([[0.4899]], device='cuda:0'))]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 58ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 56ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 60ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 59ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 57ms/step\n",
            "Named Entity =>  ('diarrhea', 'SYMPTOM') [[('https://ontology.drpawspaw.com/ADSO0000000008', 'Vomiting', 0.9960421323776245)], [('https://ontology.drpawspaw.com/ADSO0000000009', 'Diarrhea', 0.9990774393081665)]]\n",
            "prediction [(rdflib.term.Literal('Babesiosis', lang='en'),), (rdflib.term.Literal('Distempter', lang='en'),)]\n",
            "symptoms_uris [['https://ontology.drpawspaw.com/ADSO0000000008', 'https://ontology.drpawspaw.com/ADSO0000000015', 'https://ontology.drpawspaw.com/ADSO0000000005', 'https://ontology.drpawspaw.com/ADSO0000000014', 'https://ontology.drpawspaw.com/ADSO0000000009', 'https://ontology.drpawspaw.com/ADSO0000000007', 'https://ontology.drpawspaw.com/ADSO0000000006'], ['https://ontology.drpawspaw.com/ADSO0000000027', 'https://ontology.drpawspaw.com/ADSO0000000034', 'https://ontology.drpawspaw.com/ADSO0000000031', 'https://ontology.drpawspaw.com/ADSO0000000033', 'https://ontology.drpawspaw.com/ADSO0000000036', 'https://ontology.drpawspaw.com/ADSO0000000030', 'https://ontology.drpawspaw.com/ADSO0000000040', 'https://ontology.drpawspaw.com/ADSO0000000026', 'https://ontology.drpawspaw.com/ADSO0000000017', 'https://ontology.drpawspaw.com/ADSO0000000037', 'https://ontology.drpawspaw.com/ADSO0000000032', 'https://ontology.drpawspaw.com/ADSO0000000005', 'https://ontology.drpawspaw.com/ADSO0000000038', 'https://ontology.drpawspaw.com/ADSO0000000023', 'https://ontology.drpawspaw.com/ADSO0000000039', 'https://ontology.drpawspaw.com/ADSO0000000008', 'https://ontology.drpawspaw.com/ADSO0000000093', 'https://ontology.drpawspaw.com/ADSO0000000009', 'https://ontology.drpawspaw.com/ADSO0000000007', 'https://ontology.drpawspaw.com/ADSO0000000028']]\n",
            "symptom_suggestions ['Vomiting', 'Abnormal Pain', 'Fever', 'Limb Swelling', 'Diarrhea', 'coughing', 'Inflammation of Joints', 'Eye Discharge', 'Muscle Twitching', 'Thickening of nose and foot pads', 'Skin Sores', 'Excessive Saliva', 'Loss of Appetite', 'Seizures', 'Nasal Discharge', 'Lethargy', 'Head Tilt', 'Pneumonia', 'Fever', 'Involuntary Eye Movements', 'Pain', 'Paralysis', 'Vomiting', 'Breathing Difficulties', 'Diarrhea', 'coughing', 'Sneezing']\n",
            "{'request': 'My dog has been vomiting has Limb Swelling and has diarrhea', 'result_type': 'SUGGESTION', 'symptom_suggestions': {'Pain', 'Vomiting', 'Abnormal Pain', 'Fever', 'Breathing Difficulties', 'Loss of Appetite', 'Thickening of nose and foot pads', 'Limb Swelling', 'Nasal Discharge', 'coughing', 'Muscle Twitching', 'Diarrhea', 'Skin Sores', 'Lethargy', 'Head Tilt', 'Paralysis', 'Sneezing', 'Eye Discharge', 'Involuntary Eye Movements', 'Seizures', 'Excessive Saliva', 'Inflammation of Joints', 'Pneumonia'}, 'predicted_disease': None}\n",
            "--- Time elaped for single execution:  12.922767877578735  ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "r-k9ADkGrv7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "symptoms[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIcrtJycrsll",
        "outputId": "af7d4d8c-f4b8-4a90-f58f-c540f3686381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Fast Breathing', 'Nausea', 'Skin Scaling', 'Severe Weight Loss', 'Lethargy']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "sBlPtNmsDGgc",
        "V4v4SEn8D0RA",
        "yWRSUfHdDYGl",
        "cfGSSJUBGvyo",
        "x6ilvSv6XtzP",
        "Ci4b6Q6-IyJX",
        "hp4vB2bgjsqr",
        "r-k9ADkGrv7B"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7fa1aaeae5a14471b61afe3b7b8f500f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2adce70529e443ef81130c1bf7c249d6",
              "IPY_MODEL_c6d848adddd946509fe893613faa2c2a",
              "IPY_MODEL_2c381ea14db6466cbc5e33f27e315665"
            ],
            "layout": "IPY_MODEL_6bc10bd46dd94ac282a50b661655d266"
          }
        },
        "2adce70529e443ef81130c1bf7c249d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7176f289a4cc4c72a6f72be6f143d5ac",
            "placeholder": "​",
            "style": "IPY_MODEL_57a236e1360a4cb78f7c6ce637dd8f0f",
            "value": "Fetching 9 files: 100%"
          }
        },
        "c6d848adddd946509fe893613faa2c2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c26eae418f7421b8aaa92d66dc1ae54",
            "max": 9,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2be4de0d10414561b02e64fdbb313089",
            "value": 9
          }
        },
        "2c381ea14db6466cbc5e33f27e315665": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b5077b19162454699720d0e1bade77d",
            "placeholder": "​",
            "style": "IPY_MODEL_19cb0a1f92e04f1f9ef961340db5807b",
            "value": " 9/9 [00:00&lt;00:00, 353.42it/s]"
          }
        },
        "6bc10bd46dd94ac282a50b661655d266": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7176f289a4cc4c72a6f72be6f143d5ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57a236e1360a4cb78f7c6ce637dd8f0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c26eae418f7421b8aaa92d66dc1ae54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2be4de0d10414561b02e64fdbb313089": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b5077b19162454699720d0e1bade77d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19cb0a1f92e04f1f9ef961340db5807b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}